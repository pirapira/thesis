\chapter{Hypersequents in the Programming Language Haskell}
\label{ch:haskell}

\section{Summary}

 We investigate the computational meaning of hypersequents and realize
 it as a Haskell library.
 The hypersequent calculus is introduced by mathematical
 logicians~\citep{avron91,alg,metcalfe2006} in order
 to obtain cut-free deduction systems for more logics.
 In the previous chapters, we proposed using hypersequents
 for representing asynchronous or synchronous communication.
 In this chapter, we try implementing hypersequents within a programming
 language \texttt{Haskell}~\citep{marlow2010haskell}.
 Logically, a hypersequent
 means every model satisfies at least one component.
 Computationally, a hypersequent
 means every execution makes at least one component
 successful.
 We use this analogy
 for waitfreely communicating threads.  This is useful
 because typical waitfree protocols use the fact that at least one thread
 can successfully read other thread's information.
 Throughout this chapter, \texttt{Haskell} syntax is assumed%
 \footnote{Under Haskell platform,
 \texttt{cabal install waitfree} will install this library.}.
 Another purpose of this chapter is confirmation of the proof of
 \thref{th:soundness} and \thref{th:completeness} by implementing it.

\iffalse
\begin{code}
 {-# LANGUAGE TypeOperators, MultiParamTypeClasses, FunctionalDependencies, FlexibleInstances, UndecidableInstances #-}

module Control.Concurrent.Waitfree
    ( ZeroT
    , SucT
    , HNil
    , HCons
    , (:*:)
    , K
    , single
    , Thread (t, atid)
    , AbstractThreadId
    , peek
    , comm
    , follows
    , execute
    , choice
    , cycling
    , exchange
    , (-*-)
    )
    where

import Control.Concurrent (ThreadId, forkIO, killThread)
import Control.Concurrent.MVar (MVar, tryPutMVar, readMVar,
 newEmptyMVar, tryTakeMVar)
import qualified Data.Map as Map
\end{code}
\fi

\section{Introduction}

\subsection{Waitfreedom}
\label{subsect:wf}

Waitfreedom is a notion born in the theory of asynchronous
communication~\citep{lamport1979make,Herlihy99,Saks:1993vq}. Since
waitfreedom provides no synchronization among processes, it has
served as a basis for comparing different synchronization primitives.
The intuition of waitfreedom is simple: a thread cannot
wait for another thread.  Suppose there are a finite number of threads and they can
communicate using a store. They can
visit the store, put and take things on the store and then leave the
store. The problem is that the thread's movements can be arbitrarily
slow: they can delay for unspecified amount of time and the threads
have no control over the delay. Waitfreedom prohibits a thread from
waiting for another; that is, a thread cannot choose to stay at the
store until another thread comes to the store; nor can a thread
keep visiting the store until another thread comes to the
store\footnote{Thus a thread cannot busy-wait for the shared memory to
be filled.}.
The
latter restriction can be formalized as existence of a constant natural
number~$k$ so
that no thread visits the store more than
$k$-times in any execution.  So, when a thread consumes all permitted
visits to the store, the thread has to give up receiving anything yet to come
from other threads.  We can
enforce this restriction by prohibiting
looping so that each thread can make steps at most the number of lines
in its program.

This notion of waitfreedom is originally imperative: involving reads from and writes to
the memory and the ordering of events.  In other words, this description
is about
\textit{how} waitfree computation works but it is hard to deduce from
this notion \textit{what} waitfree computation computes.
The merit of using a functional programming language, especially a pure
one like Haskell, is emphasized if the program describes \textit{what}
rather than \textit{how} it computes.

Concrete examples give a useful intuition for modeling what waitfree
protocols can compute.
Suppose a thread has a value~$v$ and another has~$w$.
It is waitfreely impossible for the two threads to exchange their
possessions.  Instead, there is a waitfree protocol for them to ensure
that either $w$ is passed to the second thread or $v$ is passed to the
first thread.
In other words, only one-way communication is guaranteed.  We are going
to exploit this property when formulating waitfree computation with hypersequents.


\subsection{Implementing Hyper-Lambda Calculus}

In waitfree protocol, two threads can communicate in one-way fashion,
and it is decided at execution time which one can send information to
the other.  This phenomenon can be understood as a set of computations
of which at least one is guaranteed to be successful.  Such a set of
computations can be represented as a hypersequent.
\verb@K s a@ represents thread \verb@s@'s computation yielding a value of type \verb@a@ and
\verb@K t b@ represents thread \verb@t@'s computation yielding a value of type \verb@b@.
The waitfree protocol described at the end of Subsection~\ref{subsect:wf}
witnesses the following derivation because either \verb@t@ obtains
\verb@b@ or \verb@s@
obtains \verb@a@ after executing the protocol.
\begin{center}
 \AxiomC{\tt K t a}
 \AxiomC{\tt K s b}
 \BinaryInfC{\texttt{K t b}$\hmid$ \texttt{K s a}}
 \DisplayProof
\end{center}
However, this derivation only contains types but not programs.  In this
chapter, we are going to propose how to write a program belonging to this
type in Haskell.

From this perspective, the merit of this chapter exists in showing the
hyper-lambda calculi are implementable.
Hypersequents are roughly sequences of
typed terms.  At first, they were introduced as an apparatus for obtaining
cut-elimination theorems for more logics.
However, as we have seen in Chapters~\ref{ch:exchange}, \ref{ch:lambda}
and \ref{ch:pole}, we can obtain programming languages out of
hypersequent calculi.
Thus it is natural to try implementing the programming languages.

\section{An Example Program}

\begin{figure*}[t]
\begin{spec}
handle :: PortID -> IO Handle
handle p = withSocketsDo $ do
  s <- listenOn p
  (h,\_,\_) <- accept s
  hSetBuffering h NoBuffering
  return h
prepareHandle :: Thread t => PortID -> IO (K t Handle :*: HNil)
prepareHandle p = single $ handle p

readLine :: Thread t => t -> Handle -> IO ((Handle, String), String)
readLine th h = do
  hPutStr h $ (show $ atid th) ++ " requiring input: "
  str <- hGetLine h
  return ((h, str), str)
readH :: Thread t => PortID -> IO (K t ((Handle, String), String) :*: HNil)
readH p = prepareHandle p >>= (readLine -*- return)

printTaken :: Thread t => t -> ((Handle, String), String) -> IO ()
printTaken th ((h, selfs), peers) = do
        hPutStrLn h $ (show $ atid th) ++ " got: " ++ show (selfs, peers)
        return ()

twoPrints :: K ZeroT ((Handle, String), String) :*:
             K (SucT ZeroT) ((Handle, String), String) :*: HNil
              -> IO (K ZeroT () :*: K (SucT ZeroT) () :*: HNil)
twoPrints = printTaken -*- printTaken -*- return

rerror :: Thread t => t -> (Handle, a) -> IO ()
rerror th (h, _) = hPutStrLn h $ "Thread " ++ (show $ atid th) ++
                   " failed to read peer's input."

content ::  IO (K ZeroT () :*: K (SucT ZeroT) () :*: HNil)
content = comm (readH $ PortNumber 6000) rerror (readH $ PortNumber 6001) rerror
          >>= twoPrints
main :: IO ()
main = execute content
\end{spec}
 \caption[An example program that provides waitfree communication to two
 clients.]
 {An example program.  \texttt{:*:} delimits components.
 Library imports are omitted.
 This program spawns two threads each waiting for
 a TCP connection.  The two threads do waitfree communication, so the
 slowest thread obtains the inputs for all threads. }
 \label{fig:example}
\end{figure*}

We show an example program written using our library
(Figure~\ref{fig:example}).  This is a concrete realization of the
simplest nontrivial waitfree computation described at the end of Subsection~\ref{subsect:wf}.

\paragraph{The external behavior of the example program.}
  This program can be compiled and executed as:
\begin{verbatim}
% ghc --make -threaded waitfree_test.hs
[1 of 1] Compiling Main
Linking waitfree_test ...
% ./waitfree_test
\end{verbatim}
Then, it spawns two threads listening on port 6000 and 6001.
If we connect to the first thread on port 6000 and
give input \texttt{apple}, the first thread tries to obtain the input
given to the other
thread but since the first thread cannot wait for the second,
fails to do so and aborts.  Below, we omit the outputs of
\texttt{telnet} for clarity.
\begin{verbatim}
% telnet localhost 6000
0 requiring input: apple
Thread 0 failed to read peer's input.
\end{verbatim}
When we connect to the second thread on port 6001 and give another
input \texttt{orange}, the second thread obtains the input \texttt{apple}
given to the first thread and displays both inputs.
\begin{verbatim}
% telnet localhost 6001
1 requiring input: orange
Thread 1 got: ("orange\r","apple\r")
\end{verbatim}

\paragraph{The internal behavior of the example program.}

Each of threads 0 and 1 writes its input into the shared memory and then
tries to read the other thread's input from the shared memory.  When
a thread tries to read what the other thread has not written, the reader aborts.
The reading thread cannot wait for the other thread's write
operation.

We assume sequential consistency.  Sequential consistency is an illusion
sustained by shared memory mechanism that pretends to align all memory
operations in a totally ordered sequence.  In this case, there are
six possible such ordered sequences.
\begin{itemize}
 \item 0 writes, 0 reads, 1 writes and 1 reads.  Then, 1 reads 0's write.
 \item 0 writes, 1 writes, 0 reads and 1 reads.  Both read each other's write.
 \item 0 writes, 1 writes, 1 reads and 0 reads.  Both read each other's write.
 \item 1 writes, 0 writes, 0 reads and 1 reads.  Both read each other's write.
 \item 1 writes, 0 writes, 1 reads and 0 reads.  Both read each other's write.
 \item 1 writes, 1 reads, 0 writes and 0 reads.  Then, 0 reads 1's write.
\end{itemize}
In any of these six cases, either thread 0 obtains thread 1's input or 1 obtains
0's input.

\paragraph{The source code of the example program.}

We look at the source code of this sample program in Figure~\ref{fig:example}.

For implementing the above described external behaviour,
an obvious thing to do is to establish a TCP connection.  This is done
by a function called \verb@handle@.   The input of \verb@handle@ is a
\verb@PortID@, which is the port number.  The output of \verb@handle@ is
a \verb@Handle@, which is a TCP connection.
\begin{spec}
handle :: PortID -> IO Handle
handle p = withSocketsDo $ do
  s <- listenOn p
  (h,_,_) <- accept s
  hSetBuffering h NoBuffering
  return h
\end{spec}
Since this connection establishment should be done by a thread,
we prepare a hypersequent containing this computation.  For that, we can use
our library function~\verb@single@ that creates a hypersequent containing
a single component.
\begin{spec}
prepareHandle :: Thread t =>
                 PortID -> IO (K t Handle :*: HNil)
prepareHandle p = single $ handle p
\end{spec}

Next the program asks the user to provide an input.
In doing that, showing the name of the current thread is useful.
When a type~\verb@t@ is of the type class~\verb@Thread@, we can obtain the
thread's number via \verb@atid@ function (\verb@atid@ stands for Abstract Thread
ID).
\begin{spec}
readLine :: Thread t =>
            t -> Handle -> IO ((Handle, String), String)
readLine th h = do
  hPutStr h $ (show $ atid th) ++ " requiring input: "
  str <- hGetLine h
  return ((h, str), str)
\end{spec}
The result of this interaction is stored in a tuple \verb@((h, str), str)@,
whose left part contains things to be used locally, and whose right
part contains things to be used remotely in the other thread.
The handle \verb@h@ is kept locally at the thread while the obtained
input \verb@str@ is duplicated.

This distinction of local and remote usage is apparent in the type of
\verb@comm@ function, which provides the simplest waitfree communication.
\begin{spec}
comm :: (Thread s, Thread t, HAppend l l' l'') =>
         IO (K t (b,a) :*: l)
         -> IO (K s (d,c) :*: l')
         -> IO (K t (b,c) :*: K s (d,a) :*: l'')
\end{spec}
The \verb@comm@ function assumes that the types \verb@s@ and \verb@t@ are \verb@Thread@s and
that a hypersequent \verb@l''@ is the concatenation of \verb@l@ and \verb@l'@.
Under these assumptions, the function takes two hypersequents in \verb@IO@
monad and returns
a hypersequent in \verb@IO@ monad.
The first argument says either \verb@t@'s computation of a value of type
\verb@(b,a)@ or one of the components of the hypersequent~\verb@l@ succeeds.
The second argument says the same thing for \verb@s@'s \verb@(d,c)@ and \verb@l'@.
If any component in \verb@l@ or \verb@l'@ is successful, that component witnesses the
resulting hypersequent.  Otherwise, when
thread \verb@t@
computes \verb@(b,a)@ value and
\verb@s@ computes \verb@(d,c)@ value successfully, the \verb@comm@ function guarantees either
\verb@t@ obtains the \verb@c@ value or \verb@s@ obtains the \verb@a@ value.  The \verb@b@ value
and the \verb@d@ value are only used locally.
Without this distinction of
local and remote usage, the TCP connections would be communicated
together with the inputs so that the connections are not closed until all
threads terminates.

When we draw the type of \verb@comm@ in the hypersequent style,
 \begin{center}
  \AxiomC{\texttt{IO (K t (b,a) :*: l)}}
  \AxiomC{\texttt{IO (K s (d,c) :*: l')}}
  \BinaryInfC{\texttt{IO (K t (b,c) :*: K s (d,a) :*: l'')}}
  \DisplayProof
 \end{center}
 we see some similarities and differences from the hyper-lambda calculus
 in Chapter~\ref{ch:lambda}.  When we ignore \texttt{b} and \texttt{d},
 then the rule is the same as $ij$-com rule in Figure~\ref{fig:logic}
 except that the type of \verb@comm@ does
 not have contexts.

The final agenda is to output the obtained inputs.  This is done by the
\verb@printTaken@ function.
Similarly to \verb@readLine@ function, the \verb@printTaken@ function takes the
thread and the result of the last computation.

Everything is wrapped up into a hypersequent~\verb@content@ of the type
 \begin{center}
\verb@IO (K ZeroT () :*: K (SucT ZeroT) () :*: HNil)@
 \end{center}
and it is \verb@execute@d.
Here, \verb@ZeroT@ means thread 0 and \verb@SucT ZeroT@ means thread 1.
The content of \verb@execute@ function is explained in
the next section.

\section{Implementation of the Library}

\subsection{What is a Hypersequent}
We implemented hypersequents as heterogeneous collections by \citet{hetero}
with a slight modification.  We required each element of such a
heterogeneous collection to be a thread's computation.
We expressed threads as type level natural numbers, which is obtained by
simplifying heterogeneous lists.
Each thread is expressed as a singleton type containing a single value.
When that value is passed to the \verb@atid@ function, the function returns
an \verb@AbstractThreadId@, which is tentatively defined to be
\verb@Int@.

A type \verb@t@ can be of type class \verb@Thread@ only if
there is a constant called \verb@t@ of type \verb@t@.
\begin{code}
class Thread t where
    t :: t
    atid :: t -> AbstractThreadId
\end{code}
We declare a type called \verb@ZeroT@, which have a single value called
\verb@ZeroT@ (it is a custom of the Haskell community to use the same name
for a type and its unique constructor).
The type \verb@ZeroT@ is of type class \verb@Thread@ after defining
constant \verb@t@ to be the single value \verb@ZeroT@.
The type \verb@ZeroT@ is going to be used as the label for thread~0.
 \begin{code}
data ZeroT = ZeroT
instance Thread ZeroT where
    t = ZeroT
    atid ZeroT = 0
 \end{code}
We are going to allow users to use as many threads as they want.
After \verb@ZeroT@, we inductively define infinitely many types of type
class \verb@Thread@.  For type \verb@t@, we define a type \verb@SucT t@.
A value of \verb@SucT t@ is of the form \verb@SucT t@ where \verb@t@
is a value of type \verb@t@.
Especially, when type \verb@t@ is of type class \verb@Thread@,
the new type \verb@SucT t@ is also of type class \verb@Thread@.
The type class \verb@Thread@ requires a constant \verb@t@ of
\verb@SucT t@,
which we define to be \verb@SucT t@ where value \verb@t@ of type
\verb@t@ is the constant provided by our assumption that type \verb@t@
is of type class \verb@Thread@.
 \begin{code}
data SucT t = SucT t
instance Thread t => Thread (SucT t) where
    t = SucT t
    atid (SucT x) = succ $ atid x

type AbstractThreadId = Int
\end{code}
Using these, we have many types of type class \verb@Thread@:
\verb@ZeroT@, \verb@SucT ZeroT@ and so on.
This technique is called type level natural numbers.

Thread~\verb@t@'s computation of type~\verb@a@ is represented as \verb@K t a@.
Internally, it is a pair of thread and a computation of \verb@JobStatus a@.
A \verb@JobStatus@ can be \verb@Having a@,
\verb@Done TryAnotherJob@ or \verb@Done Finished@.
\verb@Having a@ shows the thread's computation is not yet finished and
as the intermediate result the thread have a value of type \verb@a@.
\verb@Done TryAnotherJob@ shows that the thread has stopped after
failing to read shared memory content.
\verb@Done Finished@ shows that the thread has stopped.
\begin{code}
newtype K t a = K (t, IO (JobStatus a))

data JobStatus a = Having a | Done ThreadStatus
data ThreadStatus = TryAnotherJob | Finished
\end{code}

A hypersequent is either \verb@HNil@ or \verb@HCons (K t e) l@ where \verb@l@ is a
hypersequent.  This inductive definition realizes the idea of a
heterogeneous list of threads' computations.  This construction is an
adaptation of \citep{hetero}.
First we declare a type class called \verb@HyperSequent@.
\begin{code}
class HyperSequent l
\end{code}
Second, we define the empty hypersequent called \verb@HNil@ of type
\verb@HNil@ of type class \verb@HyperSequent@.
\begin{code}
data HNil = HNil
instance HyperSequent HNil
\end{code}
Third, we define the cons operation of hypersequent.
\verb@HCons e l@ is of type \verb@HCons e l@ when \verb@e@ is of type
\verb@e@ and \verb@l@ is of type \verb@l@.  Especially, when type
\verb@l@ is of type class \verb@HyperSequent@, the type \verb@HCons (K\,t\,e) l@
is also of type class \verb@HyperSequent@ whatever the type \verb@e@ is.
\begin{code}
data HCons e l = HCons e l
instance HyperSequent l =>
  HyperSequent (HCons (K t e) l)
\end{code}
Finally, we define an abbreviation for type \verb@HCons e l@.
\begin{code}
infixr 5 :*:
type e :*: l = HCons e l
\end{code}
In our framework, every hypersequent is going to be contained in
\texttt{IO} monad
so that hypersequent derivations can contain MVar
cells. The MVar cells are used for implementing the communication
rule. Although we use MVar cells, which are capable of supporting full
synchronization among threads, we only use a limited set of functions
that only allows waitfree communication. For instance, we do not use
\verb@takeMVar@ or \verb@putMVar@ because they are blocking operations.

\subsection{How to Execute a Hypersequent}
Of course we can execute a hypersequent after we construct it.  The
execution takes three steps: preparing computations, spawning
threads and waiting for them.

After a hypersequent derivation is constructed, it is turned into a list
of local computations.  A value of type \verb@L@ is internally used to
represent a piece of
local computation.  Each piece of computation results in either \verb@TryAnotherJob@
or \verb@Finished@.  These decide whether the thread continues to operate or stops.
\begin{code}
type L = (AbstractThreadId, IO ThreadStatus)
\end{code}

Local computations are then transformed into a finite map from thread
identifiers to channels containing computations. Each thread reads the list for
its own identifier and executes the computations in the list one by one.
A type \verb@l@ can be of type class \verb@Lconvertible@ only if
a value of type \verb@l@ can be converted into a list of \verb@L@
values.
\begin{code}
class Lconvertible l where
    htol :: l -> [L]
\end{code}
Any value of type \verb@HNil@ is converted into an empty list of
\verb@L@ values.
\begin{code}
instance Lconvertible HNil where
    htol _ = []
\end{code}
A longer hypersequent is also convertible into a list of \verb@L@ values.
\begin{code}
instance (Thread t, Lconvertible l) =>
 Lconvertible (HCons (K t ThreadStatus) l) where
    htol (HCons (K (th, result)) rest) =
      (atid th, fmap jth2th result) : htol rest

jth2th :: JobStatus ThreadStatus -> ThreadStatus
jth2th (Having x) = x
jth2th (Done x) = x
\end{code}

Since a list of type \verb@[L]@ contains pieces of computation for
possibly multiple threads, the next thing is demultiplexing the list
into many lists each of which serves jobs to one thread.
We construct a list of computations for each thread.  The lists have type
\verb@JobChannel@.  The \verb@JobChannel@s are put in a finite map called
\verb@JobPool@.  The \verb@JobPool@ maps each \verb@AbstractThreadId@ to a
\verb@JobChannel@.  Later, the worker threads look at this map to take
jobs.
\begin{code}
type JobChannel = [IO ThreadStatus]

type JobPool =
    Map.Map AbstractThreadId JobChannel
\end{code}
The actual conversion is done by \verb@constructJobPool@ which acts
inductively on a list of type \verb@[L]@.
\begin{code}
constructJobPool :: [L] -> JobPool
constructJobPool [] = Map.empty
constructJobPool ((aid, action) : tl) =
  Map.insertWith (++) aid [action] rest
     where rest = constructJobPool tl
\end{code}

At last, threads are spawned.  Each thread's operation is defined in
\verb@worker@ function.  It takes two arguments.  The first argument is a
channel containing jobs.  The other argument is an MVar cell for telling
termination to the main thread.  Although the worker threads do not wait for any other
worker thread, the main thread waits for the worker threads.
The worker tries consuming jobs in the provided \verb@JobChannel@ one by
one until one succeeds or all fails.  After the worker consumes all jobs
in the provided \verb@JobChannel@, the worker puts \texttt{()} into the
provided MVar cell so that the main thread can notice that the worker is idling.
\begin{code}
worker :: JobChannel -> MVar () -> IO ()
worker [] fin = tryPutMVar fin () >>= \_ -> return ()
worker (hd : tl) fin = do
  result <- hd
  case result of
    TryAnotherJob -> worker tl fin
    Finished -> tryPutMVar fin () >>= \_ -> return ()
\end{code}

When the main thread spawns the worker threads, it has their
\verb@ThreadId@s and the termination MVar cells in a finite map.
After spawning the worker threads, the main thread waits for each
worker thread to fill the MVar and then kills the worker thread.
A value of type \verb@ThreadPool@ maps a value of
\verb@AbstractThreadId@ into a value of \verb@ThreadId@ and an MVar.
A value of type \verb@ThreadId@ is provided by the Haskell
\verb@Concurrent@ library and a value of \verb@ThreadId@ can be used to
kill a thread.  The \verb@MVar@ is filled when the worker thread finishes the
computation and then the main thread can see the worker thread has finished.
 \begin{code}
  type ThreadPool = Map.Map AbstractThreadId (ThreadId, MVar ())
 \end{code}
 The main thread uses \verb@threadSpawn@ to register a new thread in the
 thread pool using an \verb@AbstractThreadId@ and a \verb@JobChannel@.
 \begin{code}
threadSpawn :: AbstractThreadId -> JobChannel -> IO ThreadPool -> IO ThreadPool
threadSpawn aid ch p = do
    p' <- p
    fin <- newEmptyMVar
    thid <- forkIO $ worker ch fin
    return $ Map.insert aid (thid, fin) p'
 \end{code}
 Then, the main thread waits for all worker threads and kills them.
\begin{code}
waitThread :: ThreadPool -> IO ()
waitThread = Map.fold threadWait $ return ()

threadWait :: (ThreadId, MVar ()) -> IO () -> IO ()
threadWait (thid, fin) w = do
    readMVar fin
    killThread thid
    w
\end{code}
At last, everything is wrapped up into a function called \verb@execute@.
\begin{code}
execute :: Lconvertible l => IO l -> IO ()
execute ls = do
  ls >>= run . constructJobPool . htol >>= waitThread

run :: JobPool -> IO ThreadPool
run = Map.foldrWithKey threadSpawn $ return Map.empty
\end{code}
Since all hypersequents are of types belonging to type class
\verb@Lconvertible@,
they can be \verb@execute@d.

\subsection{How to Construct a Hypersequent}
Since a hypersequent represents a finite multiset of computations of which
at least one succeeds, there cannot be an empty hypersequent.  Thus, the
simplest hypersequent is a hypersequent consisting of a single component.
This can be built with \verb@single@ function.
The ingredient computation is not of type \verb@IO a@ but of type
\verb@t -> IO a@ so that the computation can display the thread~\texttt{t}'s
\verb@AbstractThreadId@.
\begin{code}
single :: Thread t => (t -> IO a) -> IO (K t a :*: HNil)
single f = return $ HCons (remote $ f t) HNil
  where remote y = K (t, fmap Having y)
\end{code}

As we want to regard a hypersequent as a multiset of components, we allow
permutation and concatenation of hypersequents.
First, \verb@exchange@ function changes the positions of the first two
components:
\begin{code}
exchange :: K t a :*: K s b :*: l -> IO (K s b :*: K t a :*: l)
exchange (HCons x (HCons y rest)) = return $ HCons y $ HCons x rest
\end{code}
Second, \verb@cycling@ functions puts the last component in the first position.
In order to implement this function, we defined a type class called
\verb@HLast@.
The type class instance \verb@HLast l a heads@ means that the hypersequent \verb@l@
is a concatenation of \verb@heads@ and a singleton list made of \verb@a@.  In
other words, the last element of \verb@l@ is \verb@a@ and the rest is \verb@heads@.
This tactic of defining a type class is taken from the paper of
heterogeneous collections~\citep{hetero}.
By \verb@exchange@ and \verb@cycling@, we can perform all permutations of
hypersequents\footnote{This implementation of permutations comes from
Tatsuya Abe's
implementation of a typed
lambda calculus for modal logic~K. Tatsuya Abe
showed the Agda~\citep{agda} implementation to the author in Tokyo.}.

First, we declare the type class \verb@HLast@.
Here we use functional dependencies~\citep{mark_jones2000} to specify
that the choice of type~\verb@l@ uniquely determines the types \verb@a@ and
\verb@heads@ that satisfies \verb@HLast l a heads@ (if there exists such
\verb@a@ and \verb@heads@ at all).
In other words, \verb@HLast@ is actually a partial function that takes
type~\verb@l@
and returns types \verb@a@ and \verb@heads@.
\begin{code}
class HLast l a heads | l -> a, l -> heads
 where hLast :: l -> (a, heads)
\end{code}
The \verb@HLast@ is defined on hypersequents inductively.
\begin{code}
instance HLast (HCons a HNil) a HNil
    where hLast (HCons x HNil) = (x, HNil)

instance (HLast (HCons lh ll) a heads) =>
  (HLast (HCons b (HCons lh ll)) a (HCons b heads))
    where hLast (HCons y rest) =
              case hLast rest of
                (x, oldheads) -> (x, HCons y oldheads)
\end{code}
Using this type class \verb@HLast@, we can define \verb@cycling@ function
which moves the last element of a hypersequent to the first position.
\begin{code}
cycling_ :: HLast l a heads => l -> HCons a heads
cycling_ lst = case hLast lst of
                (last_, heads) -> HCons last_ heads

cycling :: HLast l last heads => IO l -> IO (HCons last heads)
cycling = fmap cycling_
\end{code}
Finally, we provide concatenation of hypersequents with \verb@follows@
function.
Again, we prepare a type class called \verb@HAppend@ for this.
This technique is also from \citet{hetero}.
\begin{code}
follows :: HAppend l l' l'' => IO l -> IO l' -> IO l''
follows l0 l1 = do
  h0 <- l0
  h1 <- l1
  return $ hAppend h0 h1

class HAppend l l' l'' | l l' -> l''
 where hAppend :: l -> l' -> l''

instance HyperSequent l => HAppend HNil l l
 where hAppend HNil = id

instance (HyperSequent l, HAppend l l' l'')
    => HAppend (HCons x l) l' (HCons x l'')
 where hAppend (HCons x l) = HCons x. hAppend l
\end{code}

In addition to simple permutations of hypersequents, we have another
structural rule called external contraction.  When a hypersequent begins
with two components of the same type, they can be merged into one.  The
resulting component represents the computation of trying the two original
components one by one until one of them is successful or all components
fail.
The type of \verb@choice@ reveals that the two components of the same type
\verb@K t a@ are squashed into a single component.
\begin{code}
choice :: Thread t => K t a :*: K t a :*: l -> IO (K t a :*: l)
choice (HCons (K (_, comp0))
  (HCons (K (_, comp1)) rest)) =
  return $ HCons (K (t, result)) rest
  where
    result = do
      r0 <- comp0
      case r0 of
        Having a -> return $ Having a
        Done TryAnotherJob -> comp1
        Done Finished -> return $ Done Finished
\end{code}

It is possible to compose a piece of local computation with another
piece of computation
and obtain a new piece of local computation.
For that purpose, we prepare \verb@extend@ function.
\begin{code}
extend :: Thread t => (K t a -> IO (JobStatus b)) -> K t a -> K t b
extend trans r = K (t, trans r)
\end{code}
Such transformations of local computation can be changed into
transformations of hypersequents.
\begin{code}
infixr 4 -*-

(-*-) :: (Thread t, HyperSequent l, HyperSequent l') =>
            (t -> a -> IO b) -> (l -> IO l') ->
            HCons (K t a) l -> IO (HCons (K t b) l')
(-*-) = progress_ . extend . peek . lmaybe
  where
    lmaybe _ _  (Done x) = return (Done x)
    lmaybe f th (Having x) =  do
      y <- f th x
      return $ Having y

progress_ :: (HyperSequent l, HyperSequent l') =>
            (a -> b) -> (l -> IO l') -> HCons a l ->
            IO (HCons b l')
progress_ hdf tlf (HCons ax bl) = do
  newtl <- tlf bl
  return $ HCons (hdf ax) newtl
\end{code}

In such transformations,
the extending function must receive the result of the previous function.
This is achieved with the help of \verb@peek@ function.
\begin{code}
peek :: Thread t => (t -> JobStatus a -> IO b) -> K t a -> IO b
peek f (K (th, content)) = content >>= f th
\end{code}

The most interesting
library function \verb@comm@ makes two threads compute something, write it to
the shared memory, reads peer's write if possible and then continue
computation. We split a thread's computation into two parts: one part
up to the write and the other from the read operation. The former part is
hidden in the local computation whereas the latter part is stored in the
hypersequent so that the types of the latest computation is visible to
the user of the library.
\verb@comm@ stands for communication.  \verb@comm@ combines two
hypersequents each containing
a communicating component.
It can be used in the form
\verb@comm hypersequent1 error1 hypersequent2 error2@ where \verb@error1@ and
\verb@error2@ specifies what to do in case of read failure.
\begin{code}
comm :: (Thread s, Thread t, HAppend l l' l'') =>
        IO (HCons (K t (b,a)) l)
         -> (t -> b -> IO ThreadStatus)
         -> IO (HCons (K s (d,c)) l')
         -> (s -> d -> IO ThreadStatus)
         -> IO (K t (b, c) :*: (K s (d, a) :*: l''))
comm x terror y serror = do
  HCons (K (taT, tba)) l <- x
  HCons (K (scT, sdc)) l' <- y
  abox <- newEmptyMVar
  cbox <- newEmptyMVar
  return $ let
      tbc = comm_part tba abox cbox terror taT
      sda = comm_part sdc cbox abox serror scT
    in
    HCons (K (taT, tbc))
      (HCons (K (scT, sda)) (hAppend l l'))
    where
      comm_part tba wbox rbox err th = do
            maybeba <- tba
            case maybeba of
              Done thStatus -> return $ Done thStatus
              Having (tb, ta) -> do
                _ <- tryPutMVar wbox ta    -- writing
                cval <- tryTakeMVar rbox   -- reading
                case cval of
                  Nothing -> do
                    terror_result <- err th tb
                    return $ Done terror_result
                  Just cva -> return $ Having (tb, cva)
\end{code}
In the sixth and seventh lines counted from the bottom, both threads
perform write and then read so that at least one
direction of communication is successful.

\section{Capturing Waitfreedom}

Since this chapter presents a library for waitfree computation, it is
desirable that only waitfree computation can be implemented using this
library (soundness), and all waitfree computation can be implemented
using this library (completeness).

\subsection{Soundness}

In the library code, the worker threads only use non-blocking \verb@MVar@
operations (a blocking operation \verb@readMVar@ is used but by the main thread).
This ensures that only waitfree computation can be implemented using this
library unless the user explicitly uses other communication primitives from outside
of this library.

\subsection{Completeness}

\begin{figure*}[t]
 \small
\begin{spec}
failOut :: t -> a -> IO ThreadStatus
failOut _ _ = return TryAnotherJob
hOwnId = single $ return (t, t)
putWithName :: Thread t => t -> String -> IO ()
putWithName th content = putStrLn $ "Thread " ++ (show $ atid th) ++ ": " ++ content
putResult :: Thread t => t -> String -> IO ThreadStatus
putResult th str = do
  putWithName th $ "obtained " ++ str
  return Finished
putOneResult :: (Thread t, Thread s) => s -> t -> IO ThreadStatus
putOneResult owner content = putResult owner $ show $ [atid content]
putTwoResults :: (Thread s, Thread t, Thread u) => u -> (s,t) -> IO ThreadStatus
putTwoResults owner (c0, c1) = putResult owner $ show $ [atid c0, atid c1]
two :: (Thread s, Thread t) => IO (K s (s, t) :*: (K t (t, s) :*: HNil))
two = comm hOwnId failOut hOwnId failOut
twoBeforeComm :: IO (K ZeroT ((ZeroT, FirstT), (ZeroT, FirstT)) :*:
                     K FirstT ((FirstT, ZeroT), (FirstT, ZeroT)) :*: HNil)
twoBeforeComm = two >>= (duplicateTwo -*- duplicateTwo -*- return)
  where duplicateTwo _ x = return (x,x)
printThreeResults0 :: (Thread u, Thread s, Thread t, Thread v) =>
                      u -> (s,(t,v)) -> IO ThreadStatus
printThreeResults0 owner (c0, (c1, c2)) = putResult owner $ show $
  [atid c0, atid c1, atid c2]
printThreeResults1 owner ((c0, c1), c2) = putResult owner $ show $
  [atid c0, atid c1, atid c2]
three :: IO (K FirstT ThreadStatus :*: K SecondT ThreadStatus :*:
             K ZeroT  ThreadStatus :*: K SecondT ThreadStatus :*: HNil)
three = comm (cycling $ comm twoBeforeComm putTwoResults hOwnId failOut)
        putTwoResults hOwnId failOut
        >>= (printThreeResults1 -*- printThreeResults0 -*- printThreeResults1
        -*- printThreeResults0 -*- return)
twoLast :: IO ((K FirstT ThreadStatus) :*: (K SecondT ThreadStatus) :*: HNil)
twoLast = comm hOwnId putOneResult hOwnId putOneResult >>=
          (putTwoResults -*- putTwoResults -*- return)
twoMid :: IO ((K ZeroT ThreadStatus) :*: (K SecondT ThreadStatus) :*: HNil)
twoMid = comm hOwnId putOneResult hOwnId failOut >>=
         (putTwoResults -*- putTwoResults -*- return)
main = execute (three `follows` twoMid `follows` twoLast)
\end{spec}
 \caption{An implementation for participating set problem~\citep{participating} for three
 threads.}
 \label{fig:complete}
\end{figure*}


We show the completeness by solving a problem called
the participating set problem appearing in \citet{participating}.
In the participating set problem, threads obtain no input except their own
id's.
Each thread~$i$ outputs a set~$S_i$ of thread id's satisfying the
following conditions:
\begin{enumerate}
 \item $i\in S_i$,
 \item $S_i\subseteq S_j$ or $S_j\subseteq S_i$ for any $i,j$
 \item $i\in S_j$ implies $S_i\subseteq S_j$.
\end{enumerate}
We can solve any waitfree protocol by repeating solving a finite number
of the participating set problem instances.

Intuitively, this can be achieved by making every thread write its id to
the shared memory and then read others' writes from the shared memory.
The results are determined by the speed competition among the threads.
Thread~$i$'s output~$S_i$ contains those threads that has already
written by the time $i$ reads.
However, we have to decompose this intuitive solution into pieces of
two-thread communication in order to implement the solution using our library.

As an example, we treat the three-thread case.  The three threads are
named \verb@ZeroT@, \verb@FirstT@ and \verb@SecondT@.

First, each thread computes its own id and stores it in a tuple.
\begin{spec}
hOwnId :: Thread t => IO (K t (t, t) :*: HNil)
\end{spec}
Then, \verb@ZeroT@ and \verb@FirstT@ compete.  This results in the
following hypersequent, which means that either \verb@ZeroT@ obtains
\verb@FirstT@'s id or \verb@FirstT@ obtains \verb@ZeroT@'s id.
\begin{spec}
IO (K ZeroT (ZeroT, FirstT) :*: K FirstT (FirstT, ZeroT) :*: HNil)
\end{spec}

Before \verb@SecondT@ is put in consideration, we make the first two threads
to duplicate their possessions so that they can retain the possessions
and communicate the possessions at the same time.
\begin{spec}
twoBeforeComm :: IO ( K ZeroT ((ZeroT, FirstT), (ZeroT, FirstT)) :*:
                      K FirstT ((FirstT, ZeroT), (FirstT, ZeroT)) :*:
                      HNil)
\end{spec}

After \verb@SecondT@ competes with \verb@ZeroT@, we obtain the following
hypersequent.  \verb@FirstT@'s possessions are not changed, but \verb@ZeroT@ and
\verb@SecondT@ tries to exchange their possessions.
\begin{spec}
three__ :: IO
  (K FirstT ((FirstT, ZeroT), (FirstT, ZeroT)) :*:
   K ZeroT ((ZeroT, FirstT), SecondT) :*:
   K SecondT (SecondT, (ZeroT, FirstT)) :*: HNil)
\end{spec}
Finally, \verb@SecondT@ competes with \verb@FirstT@.  This makes a hypersequent with
any component containing all thread's ids.  Since at least one component succeeds,
at least one thread obtains all threads' ids.

However, the participating set problem requires more.  The following
output does not conform to the problem although one thread has obtained
all threads' ids.  Either $S_0 \subseteq S_2$ or $S_2\subseteq S_0$ is
required but neither is satisfied.
\begin{verbatim}
Thread 1: obtained [1,0,2]
Thread 0: obtained [0]
Thread 2: obtained [2]
\end{verbatim}
In this case threads 0 and 2 have to compete in order to decide which
is slower.  In general, either \verb@ZeroT@--\verb@SecondT@ or
\verb@FirstT@--\verb@SecondT@ competition is required so we do both and
concatenate the three competitions as one derivation.
Both of the two thread competition are similar to the simplest nontrivial
waitfree protocol already explained so we omit the details.  The whole
program is shown in Figure~\ref{fig:complete}.
\begin{spec}
main :: IO ()
main = execute $three `follows` twoMid `follows` twoLast
\end{spec}


\section{Comparison with G\"odel-Dummett Logic}


\subsection{Similarity}

There are many functional programming languages (OCaml, Haskell, Clean
etc.) that employ intuitionistic logic for their type systems.
The hypersequent formulation that we employ is very similar to that of
G\"odel-Dummett logic, which is among
intermediate logics~\citep{umezawa} between intuitionistic and
classical logics.

G\"odel-Dummett logic~\citep{dummett59} was originally introduced as a logic
obtained by adding axioms of the form
$(\varphi\rightarrow\psi)\vee(\psi\rightarrow\varphi)$ to
intuitionistic propositional logic.

\citet{avron91} proposed an alternative formulation of the same logic.
 Instead of the axioms
 $(\varphi\rightarrow\psi)\vee(\psi\rightarrow\varphi)$, he included
the ``communication rule'' on hypersequent derivations:
\begin{center}
 \AxiomC{$\hyper_0\hmid {\Gamma_0, \Delta_0} \vdash {\varphi_0}$}
 \AxiomC{$\hyper_1\hmid {\Gamma_1, \Delta_1} \vdash {\varphi_1}$}
 \BinaryInfC{$\hyper_0\hmid\hyper_1\hmid {\Gamma_0},{\Delta_1}\vdash
 {\varphi_0}\hmid {\Gamma_1},{\Delta_0}\vdash {\varphi_1}$}
 \DisplayProof
\end{center}
where $\Gamma_i$ and $\Delta_i$ are finite sets of formulae.
This communication rule inspired the author to model waitfree computation using
hypersequents by using a rule
\begin{center}
\begin{center}
 \AxiomC{$\hyper_0\hmid\mathtt{K\, t\, a}$}
 \AxiomC{$\hyper_1\hmid\mathtt{K\, s\, b}$}
 \BinaryInfC{$\hyper_0\hmid\hyper_1\hmid \mathtt{K\, t\, b}\hmid \mathtt{K\, s\, a}$}
 \DisplayProof
\end{center}
\end{center}

\citet{avron91} has proved that a  sequent
derivable with cut rules is also provable without cuts
(Gentzen's proof adopted).
However, he still wanted to see what the corresponding typed lambda
calculus looks like, saying ``it seems to us extremely important to determine the exact
computational content of them~[intermediate logics]---%
and {to develop corresponding `$\lambda$-calculi'} ''~\citep{avron91}.
Since we are working on a constructive logic, it is natural to ask what
chooses the left or right for a term of the type
$(\varphi\supset\psi)\vee(\psi\supset\varphi)$.
The answer is actually the execution time scheduling among threads.


\subsection{Difference}

G\"odel-Dummett logic does not have modalities.  Its formulae are the
same as intuitionistic or classical logic.  The only difference of
G\"odel-Dummett logic from intuitionistic logic is that more formulae
are provable in G\"odel-Dummett
logic than in intuitionistic logic.  So, if we want to encode G\"odel-Dummett
logic faithfully, we do not have to introduce modalities such as \verb@K t@.
Even without such modalities, we can regard each component of a
hypersequent as a thread. However,
If we abandon modalities and regard components of a hypersequent as
threads, the participating set problem cannot be solved because the communication rule
splits the threads into two separate groups and any prior
inter-group communication is prohibited.

This restriction would be OK if we were only interested in making sure that
at least one thread knows every other thread.  However, since the
participating set problem does not allow any two threads to be ignorant
of each other, the \verb@comm@ rule cannot provide enough communication power
to solve the participating set problem for the components of a
hypersequent if we do not have the modalities.

\section{Related Work}

\subsection{Computational Content of Hypersequents}


\citet{parallel} developed a so-called parallel dialogue games.  It is
based on Lorenzen dialogue .  It is basically proof searching from
bottom to up with player supposed
to know the answer proof tree and opponent directing for different parts
of the proof.  Since the game rules are translation of hypersequent
calculus, the global game state has local parts.  And the player can,
for example, duplicate a local part into two.  This is essentially
different from the typed lambda calculi computation because the games
give meanings to only normal form proofs while the typed lambda calculi
give meanings to all proofs.

\subsection{High Level Treatment of Concurrency}

Erlang~\citep{erlang} is a programming language designed for concurrency
using the Actor model by~\citet{actor}.
Since the Actor model separates the sender of a message from the message
itself, it provides some asynchrony.  However, in the Actor model and in
Erlang, a thread can wait for a message.  Thus, these do not provide
waitfree communication.

% Newsqueak

Asynchronous $\pi$-calculus by \citet{api} is a fragment of
$\pi$-calculus.  It is
similar to the Actor Model in that it is impossible to do something
after sending a message.  However, asynchronous $\pi$-calculus
allows a process to wait for a channel to deliver a message.  Thus,
asynchronous $\pi$-calculus is not waitfree.

Concurrent ML~\citep{concurrentML} ``provides a high-level model of
concurrency.''  However, its basic communication primitives
\texttt{accept} and \texttt{send} are blocking operations so that there
is no fragment of the language capturing waitfreedom.

\citet{Brown} proposed a combinator library for message-passing
programming in Haskell.  However, this work is also for synchronous
communication with blocking operations and there is no way of
obtaining a waitfree
fragment of this library.

Join patterns (first proposed as the join calculus by \citet{join}) allow
consuming messages from a group of
channels simultaneously.  This involves waiting for a group of channels
so that join patterns do not capture waitfreedom either.

In multi-core and multi-processor environment, synchronous communication
is time-consuming.  Compared with
synchronous communication, waitfreedom can be implemented in a less
time-consuming manner.  Since our library
interface captures waitfreedom, it is possible to build a less
time-consuming implementation for this interface although the current
implementation relies on MVar cells, which internally use spin locks.

\section{Future Work and Conclusions}

\citet*{alg} showed a class of logics can be defined using
hypersequent derivation rules.  It will be interesting to see what kind
of computation is represented by these logics.

The library presented in this chapter deals with both waitfree
communication and thread management.  It would be better if we dealt with
these different tasks separately.
We seek to expand the notion of threads into more complicated
processes so that we can treat dynamic forking and merging threads rather
than a finite set of threads specified at compile-time.

Although we were able to implement the participating set problem,
the implementation is not short nor symmetric.
There seems to be room for more elegant abstraction for symmetric
protocols such as
a rule containing more than two threads and a more direct embedding of the
participating set problem.  The main challenge is encoding the
$n$-party participating set problem in the Haskell type system uniformly
with respect to $n$.

Moreover, in order to obtain performance, we have to change the runtime
system of Haskell.  Since the current implementation uses MVar mechanism
that involve spin-locks, it must be unnecessarily slow.  In theory, waitfree
computation can be implemented without locks.  In practice also, with
sequential consistency, we can implement waitfree computation without
locks.

One possible extension of our library is addition of an ``all-possible-results''
mode.  When we execute a program under this mode, the library explores
all possible executions and returns what can happen.

Notwithstanding, since we have shown that hypersequent based waitfree computation
is implementable.  This at least supports the statement of
\thref{th:soundness} in Chapter~\ref{ch:lambda}.
Also as a side-effect, we obtained a type-checking implementation of a
hyper-lambda calculus.
