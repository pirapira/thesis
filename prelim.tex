\section{Preliminaries}

We use set-theoretic concepts such as sets, relations and functions.
  $\powerset X$ denotes the powerset of $X$.
  In any case, we consider countably infinite set of logical formulae.
  A logic is a set of logical formulae, thus, there are at most
  $\powerset{\mathbb N}$ different logics.
  Likewise, we consider at most countably many proofs and lambda terms.
  In the classical realizability argument in Chapter~\fix{},

  We denote partial maps as graphs.
  For example, ${0\mapsto M, 1\mapsto N}$ denotes a partial map that
  maps 0 to $M$ and 1 to $N$.
  We also denote partial maps as sequence whose index is the domain.
  For example, $(y_i)_{i\in I}$ denotes the partial mapping that maps
  $i\in I$ to $y_i$.
  For partial mappings whose domains are disjoint, we use $\sqcup$ to
  denote the union.
  When $f$ and $g$ have disjoint domains, $f\sqcup g$ denotes the
  partial map that maps $x$ to $f(x)$ or $g(x)$ when one of these is
  defined, or otherwise to nothing.

  For a symbol or a sequence of symbols $p$,
  $p^+$ denotes repetition of $p$ more than zero times and
  $p^*$ denotes repetition of $p$ more than or equal to zero times.
  We use BNF (Backus Naur Form) for giving inductive definitions for sets
  of sequents of symbols.
  For example, when we say we define formulae~$\phi$ by BNF:
  \[
   \phi::= \bot \mid p\mid (\phi\imp\phi)
  \]
  where $p$ is a propositional variable, we actually define a set $\Phi$
  of formulae which is the smallest set such that
  \begin{itemize}
   \item $\bot$ is in $\Phi$
   \item each propositional variable is in $\Phi$ and
   \item if $\phi$ and $\psi$ are in $\Phi$, then $(\phi\imp\psi)$ is in $\Phi$.
  \end{itemize}
  and then we declare that we call elements of $\Phi$ formulae.
  We use ``iff'' as an abbreviation for ``if and only if.''

  \fix{hmm}
  In the first chapters, agents appear.  Later, processes.  Processes
  are special agents.

  \fix{sequence notation}

  \fix{substitution}

  \fix{bound variable: with graphical notation}


\section{History}

Hereby, we briefly review the history of the mathematical logic to the
extent relevant to the content of this thesis.
We confine ourselves to the developments ofpropositional logic and
ignore anything related to predicate logic or formalizing mathematics on
logics.

\subsection{Birth of Formal Logic}

Implication is an important concept.
Different treatments of implication has taken place in the realm of
mathematical logic.
One treatment of implications called the material implication.
The material implication can be traced back at least to
Frege's \textit{Begriffsschrift}~\citep{frege} in 1879.  There, in the
section called
``conditionality,''
he begins by establishing four cases~\citep[p.~13]{frege}:
\begin{enumerate}
 \item $A$ is affirmed and $B$ is affirmed\,;
 \item $A$ is affirmed and $B$ is denied\,;
 \item $A$ is denied and $B$ is affirmed\,;
 \item $A$ is denied and $B$ is denied.
\end{enumerate}
Then he defines a notation involving $A$ and $B$
\[
\BGassert\BGconditional{B}{A}
\]
which ``stands for the
judgment that \textit{the third of these possibilities does not take
place, but one of the three other does\footnote{The emphasis is by
the English translation~\citep[p.~14]{frege}.}.''}
In the contemporary common mathematical terminology, this is equivalent
to stating $B$ implies $A$.
It was Russell who called this implication the material implication,
according to~\citet{sep-conditionals}.
After his notation, we still use $\vdash$ for judgements in our formal systems.

It is worth noting that Frege used the notation for implication to define conjunction and
disjunction in the formal system, not the other way around.  Even in
today's treatments of
like algebraic semantics or categorical semantics, implication is
fundamental (orders in algebraic semantics, morphisms in categorical
semantics) and other logical connectives
can be defined using implication.

\subsection{Intuitionistic Propositional Logic}

The intuitionistic propositional logic is a logic, that is, a set of
logical formulae.
Although the name ``intuitionistic'' comes from Brouwer's intuitionism,
the original claims of intuitionism are unrelated to this thesis.
Brouwer was skeptical about the value of formalization of
mathematics and considered the studies of formal axiomatized logic as
``mathematics of the second and third order''
~\citep[p.~10]{stigt1998}.
Nontheless, Brouwer approved publication of his student Arend Heyting's
work on defining a set of logical formulae as the intuitionistic
propositional logic.

\subsubsection{Heyting}
In 1930, Heyting~\cite{heyting1930} developed a deduction system for the
intuitionistic propositional logic.  The presentation is similar to
the contemporary one except some notational differences.  The
logical connectives are the same $\{\wedge,\vee,\supset,\neg\}$ as today, the
first three binary and the last unary%
\footnote{Though, in this thesis, we prefer using nullary $\bot$ instead of unary
$\neg.$}%
.  Logical formulae are constructed
using these connectives and the variables.
Although he already sometimes used parentheses, he still used more points
around outer connectives (e.g. $b\supset \kern -3pt\cdot \kern 2pt  a
\supset b$ indicates the left $\supset$ is outer and the right one is
inner), but we use parentheses here.
These formulae are axioms~\cite{heyting1930}, i.e. assumed to be ``correct formulae'':
\begin{description}
 \item[2.1.] $a\supset (a\land a)$
 \item[2.11.] $(a\land b)\supset (b\land a)$
 \item[2.12.] $(a\supset b)\supset (a\land c)\supset (b\land c)$
 \item[2.13.] $((a\supset b)\land (b\supset c))\supset (a\supset c)$
 \item[2.14.] $b\supset (a\supset b)$
 \item[2.15.] $(a\land (a\supset b))\supset b$
 \item[3.1.] $a\supset (a\lor b)$
 \item[3.11.] $(a\lor b)\supset (b\lor a)$
 \item[3.12.] $((a\supset c)\land(b\supset c))\supset ((a\lor b)\supset
      c)$
 \item[4.1.] $\neg a\supset (a\supset b)$
 \item[4.11.] $((a\supset b)\land (a\supset \neg b)) \supset \neg a$
\end{description}
Moreover, there are methods~\cite{heyting1930} to recognize more formulae to be correct
formulae:
\begin{description}
 \item[1.2.] If $a$ and $b$ are correct formulas, then $a\land b$ is a correct
       formula.
 \item[1.3.] If $a$ and $b$ are correct formulas, then $b$ is a correct formula.
\end{description}

Following the intuitionists' belief
that ``it is in principle impossible to set up a system of formulas that
would be equivalent to intuitionistic mathematics, for the
possiblitities of thought cannot be reduced to a finite number of rules
set up in advance,''~\citep{heyting1930} he did not pursue completeness
results for the
deduction system%
\footnote{Except in a remark mentioning Glivenko's theorem.}%
.
However, he used a form of semantics in order to show that
each axiom is independent from the other axioms and that
the excluded middle is unprovable.
For example, in order to refute the excluded middle, he used the
following tables:\\
 \begin{center}
  \begin{tabular}{c|ccc}
   $\supset $& 0  & 1  & 2 \\ \hline
   0 & 0 & 0 & 0 \\
   1 & 1 & 0 & 1 \\
   2 & 2 & 0 & 0
  \end{tabular}
  \hfill
  \begin{tabular}{c|ccc}
   $\wedge $& 0 & 1& 2\\ \hline
   0 & 0 & 1 & 2\\
   1 & 1 & 1 & 1\\
   2 & 2 & 1 & 2\\
  \end{tabular}
  \hfill
  \begin{tabular}{c|ccc}
   $\vee$& 0 & 1 & 2\\ \hline
   0 & 0 & 0 & 0 \\
   1 & 0 & 1 & 2 \\
   2 & 0 & 2 & 2\\
  \end{tabular}
  \hfill
  \begin{tabular}{c|ccc}
   $\neg $& 0 & 1 & 2\\ \hline
   & 1 & 0 & 1\\
  \end{tabular}
 \end{center}
When we assign one of $\{0,1,2\}$ to each propositional variable, we
can extend the assignment to all formulae using these tables.
Under any assignment to propositional variables, all of Heyting's axioms
are assigned 0.  Moreover, these tables have two formidable properties:
\begin{enumerate}
 \item $0\land 0 = 0$
 \item $0\supset a$ has the value 0 only when $a = 0$.
\end{enumerate}
From these, all formulae provable in Heyting's deduction system
receive the value~0 under any assignments to propositional variables.
However, if we assign 2 to $a$, $\neg \neg a\supset a$
is assigned 2.  Note that the natural order between natural numbers
plays no rule here: it is not relevant that 2 is larger than 1 or 0 is
less than 1 (in other places he uses ``all positive and negative
whole numbers and 0'').  Although Heyting used natural numbers here, he
already obtained the notion of what is called Heyting algebra today.
That is why we can refute Dummett's
axiom $(p\supset q)\lor (q\supset p)$ in the same method.

\subsubsection{G\"odel}
In 1932, G\"odel published a short note~\cite{godelprop} on intuitionistic propositional
logic, where he proved two theorems: that the intuitionistic propositional logic
cannot be seen as a many-valued logic and that
there are infinitely many propositional logics between the
intuitionistic propositional logic and the ``ordinary'' (in today's
terminology, classical) propositional logic.  The second result is the
first contribution to the realm of intermediate logics (as
Troelstra~\cite[p.~223]{goedelcollected} put it).
For distinguishing those intermediate logics and the intuitionistic
propositional logic, he uses a formula~$F_n$ for each positive natural
number~$n$:
\[
 F_n = \bigvee_{1\le i < k\le n}\left((a_i\supset a_k) \land (a_k\supset a_i)\right)\enspace.
\]
On an $n$-element chain, $F_{n+1}$ is valid while $F_n$ might not be
satisfied.  As a result, no $F_n$ is valid in the intuitionistic
propositional logic.
Although Troelstra~\cite[p.~223]{goedelcollected} writes ``the reasons
for studying intermediate logics are mainly technical,'' we find that
one typical intermediate logic, G\"odel--Dummett logic, has a
computational interpretation which has already been known: waitfreedom.
The connection between G\"odel--Dummett logic and waitfreedom will be
treated in Chapter~\ref{ch:lambda}.

Among the formulae~$F_n$,
especially, the formula~$F_2$, which is $(a_1\supset a_2)\land (a_2\supset
a_1)$, brings contradiction into classical or intuitionistic logic.  If we
had $(p\land q)\supset p$ and $F_2$ as theorems in a logic closed under
substitution and modus ponens,
$a_1\supset a_2$ is also a theorem and all formulae would be theorems or
there would be no theorems.
% In Chapter~\ref{ch:exchange} we investigate a logic with an axiom similar to $F_2$:
% $(\phi\limp\psi)\otimes(\psi\limp\phi)$, where $\otimes$ is the
% multiplicative conjunction and $\limp$ is the linear implication.  Such
% attempt has been made possible by the development of linear and
% substructural logics explained below in \ref{linear}.
In short, in some of those settings, $(p\otimes q)\limp p$ is
not a theorem because we are not
allowed to throw away $q$.

\subsection{The Brouwer--Heyting--Kolmogorov Interpretation of Logical Connectives}

Thanks to Heyting~\cite{heyting1930}, we have a formal
characterization of the intuitionistic implication.
In 1932, Kolmogorov~\cite{kolmogorov1932} introduced a ``calculus of problems''
that coincides with the intuitionistic propositional logic.
In the calculus of problems, the logical connectives $\vee, \wedge,
\supset$ connects problems to form another problem.
 \begin{quote}
  If $a$ and $b$ are two problems, then $a\land b$ designates the
  problem ``to solve both problems $a$ and $b$,'' while $a\lor b$
  designates the problem ``to solve at least one of the problems $a$ and
  $b$.''  Furthermore, $a\supset b$ is the problem ``to solve $b$
  provided that the solution for $a$ is given'' or, equivalently, ``to
  reduce the solution of $b$ to the solution of
  $a$.''  \ldots $\neg a$ designates the problem ``to obtain a
  contradiction provided that the solution of $a$ is
  given.''~\cite[p.~329]{kolmogorov1932}
 \end{quote}
 He continues to validate Heyting~\cite{heyting1930}'s axioms, and the
 deduction rules with respect to the problem calculus interpretation.

 Furthermore, he finds a rational for not including the law of excluded
 middle $a\lor \neg a$ by saying ``one must possess a general method
 either to prove or to reduce to a contradiction any proposition.  If
 our reader does not consider himself to be omniscient, he will probably
 determine that the formula cannot be found on the list of problems
 solved by him.''~\cite{kolmogorov1932}
 This argument is enough to reject Dummett's axiom $(a\supset b)\lor
 (b\supset a)$ because one must possess a general method, given any two
 problems, to
 determine whether
 one problem can be reduced to the other or the other way around.
 For the computational meaning of the law of excluded middle, we have to
 wait until 1990's.
 And the computational interpretation of Dummett's axiom is
 presented in Chapter~\ref{ch:lambda} in this thesis.

The
Brouwer--Heyting--Kolmogorov interpretation reads: ``a proof of the
implication $\varphi\supset\psi$ is a construction which permits us to
 transform any proof of $\varphi$ into a proof of $\psi$''~\fix{cite}.
The BHK
interpretation does not specify what is a proof or what kind of
transformation witnesses implication.
Notwithstanding, there is a
traditional realization of proofs as lambda terms where the
introduction rule of implication is realized as the lambda abstraction
and the elimination rule of implication is realized as the application
of lambda terms.
This encoding has a formidable property:
the reductions in the lambda calculus corresponds to the reductions of
proofs that
removes detours.
For example, suppose a natural deduction proof introduces an implication and then
immidiately eliminate the
implication.  This proof can be encoded as a lambda term whose outermost
structure is a $\beta$-redex: $(\lambda x. M)N$.
 The result of the $\beta$-reduction $M[N/x]$ encodes a proof tree using
 the same assumptions as the original and concluding the same formula as
 the original, yet without the aforementioned detour.
 a proof of implication allows transformation of proofs
by means of
substitution.  The encoding of proofs as lambda terms is traditionally
called the Curry--Howard isomorphism.
% The situation is similar to axiomatized geometry where lines and points
% could be tables and chairs \fix{(cite)}.
The former explanation of implications is justified in classical propositional logic.
The latter view is most naturally materialized in the situation of
intuitionistic propositional logic.  The last century saw their
generalization called intermediate logics~\citep{umezawa} (or superintuitionistic
logics), of which a typical example is called G\"odel--Dummett logic.
In this thesis we investigate the Curry--Howard isomorphism for
G\"odel--Dummett logic.  The G\"odel--Dummett logic validates formulas
of the form $(\phi\imp\psi)\lor(\psi\imp\phi)$, which is known as
Dummett's axiom.  We add a construct to the simply typed lambda calculus
that witnesses Dummett's axiom.
After the intermediate logics, the generalization went further to
substructural
logics~\citep{residuated}, which contains all the intermedaite logics as
well as the
(intuitionistic) multiplicative
additive fragment of linear logic.
In the later chapter, we make a typing system that lacks contraction and
weakening, which falls outside of intermediate logics but within
substructural logics albeit it has quantification over propositional variables.


\subsection{Natural Deduction and Sequent Calculus}

\subsubsection{Gentzen}

\fix{Cite Genztzen}

\subsubsection{Prawitz}

\fix{cite prawitz}


\subsection{Curry--Howard Isomorphism}

The Curry--Howard isomorphism is originally a correspondence between
intuitionistic propositional logic proofs and typed lambda terms, but
the term has obtained a more general meaning containing
correspondence between proofs and programs in general.
\citet{curryhoward} is a comprehensive reference on the topic,
containing lots of historical and bibliographical remarks.

\subsubsection{Curry}
The Curry--Howard isomorphism is originally the correspondence of
the typed lambda terms and the proofs of the implicational fragment of
the intuitionistic propositional logic.
According to \citep{curryhoward}, the first explicit statement of the
correspondence appears
in the retiring presidential address to the Association for Symbolic
Logic titled ``the combinatory foundations of mathematical
logic''~\cite{curry1942}.
The footnote~28 of \citep{curry1942} reads:
 \begin{quote}
  Note the similarity of the postulates for $F$ and those for $P$.  If
  in any of the former postulates we change $F$ to $P$ and drop the
  combinator we have the corresponding postulate for $P$.
 \end{quote}
 For example, there is Postulate~(PC):
\[
 \vdash (\beta\supset (\alpha\supset \gamma))\supset (\alpha \supset
 (\beta\supset \gamma))\enspace.
\]
And a corresponding Postulate~(FC):
\[
 \vdash F(F\beta(F\alpha\gamma))(F\alpha(F\beta\gamma))C
\]
 where $C\equiv \lambda^3 xyz\cdot xzy$ and $F\alpha\beta$ shows the
 functional character of a
 function from $\alpha$ to $\beta$.  The postulate~(FC) is said to
 state the functional character of $C$.
 In this thesis, we use sequent style natural deduction system so that these
 postulates can be derived as
 \[
 \AxiomC{}
 \useq{\tj{x}{\beta\supset (\alpha\supset \gamma)}}{\tj{x}{\beta\supset
 (\alpha\supset \gamma)}}
 \AxiomC{}
 \useq{\tj{z}{\beta}}{\tj{z}{\beta}}
 \bseq{\tj{x}{\beta\supset (\alpha\supset \gamma)},
 \tj{z}{\beta}}{\tj{xz}{\alpha\supset\gamma}}
 \AxiomC{}
 \useq{\tj{y}{\alpha}}{\tj{y}{\alpha}}
 \bseq{\tj{x}{\beta\supset (\alpha\supset \gamma)},
 \tj{z}{\beta},\tj{y}{\alpha}}{\tj{xzy}{\gamma}}
 \useq{\tj{x}{\beta\supset (\alpha\supset
 \gamma)},\tj{y}{\alpha}}{\tj{\lambda z. xzy}{\beta\supset\gamma}}
 \useq{\tj{x}{\beta\supset (\alpha\supset
 \gamma)}}{\tj{\lambda y.\lambda z. xzy}{\alpha\supset(\beta\supset\gamma)}}
 \useq{}{\tj{\lambda x.\lambda y.\lambda z. xzy}{{(\beta\supset (\alpha\supset
 \gamma))}\supset (\alpha\supset(\beta\supset\gamma))}}
 \DisplayProof \enspace .
 \]
 Indeed, the last sequent associates the term $\lambda x.\lambda
 y.\lambda z. xzy$, which is the combinator $C$, to the logical formula
 $(\beta\supset (\alpha\supset
 \gamma))\supset (\alpha\supset(\beta\supset\gamma))$.

The
precise statements for the correspondence appear in
\citet[9E]{curry1974combinatory}, a section titled ``analogies with
propositional algebra.''
\fix{cite the statements}

There, Curry notes the correspondence is not isomorphism\fix{or, in
sorenzen's book ???}  We take a finer approach.

\citet{landin1965} noticed some similarities between ALGOL~60 syntax and
the untyped lambda calculus.

Pfenning

Davies--Pfenning S4 A Modal Analysis of Staged Computation

Curien

Murphey 7

Game


\subsection{G\"odel--Dummett Logic}

\subsubsection{Dummett}
Dummett~\cite{dummett59} considers a semantics on $\{0,1,2,\ldots,\omega\}$ where
$\wedge$ is interpreted as max, $\vee$ as min, $\bot$ as $\omega$
and conjunction~$\wedge$
 and implication as a
function~$\hat\supset$ where%
\footnote{To be precise, Dummett did not use absurdity~$\bot$ but negation~$\neg$
although they are interdefinable in existence of implication~$\supset$
and conjunction~$\wedge$.}
\begin{align*}
 x \hat\supset y= \begin{cases}
		    0 &\text{ if } x\ge y \\
		    y &\text{ if } x < y
		  \end{cases}\enspace.
\end{align*}
He axiomatized the logic by adding $(p\supset q)\lor(q\supset p)$ on top
of the axioms for the intuitionistic propositional logic.
Thomas~\citep{thomas1962} axiomatized
 the logic on $n$-element chains using Dummett's axiom and the
 formula~$F_{n+1}$,
 which is rewritten using only implications but no disjunction or conjunction.

\subsubsection{Sonobe}
\fix{say something}

\subsubsection{Avron}
Hypersequent


\subsubsection{Later Developments}

\subsection{Waitfreedom}

\subsubsection{Lamport}

\subsubsection{Gafni}

\subsubsection{Herlihy and Shavit}

\subsubsection{Saks and Zaharoglow}

\subsection{Superintuitionistic Logics}

A logic is a set of logical formulae which is closed by
substitution and modus ponens.
 A superintuitionistic logic is a logic
that contains the intuitionistic propositional logic.  A logic is consistent when it does not
contain all logical formulae.

Troelstra~\cite[p.~223]{goedelcollected} writes ``the reasons
for studying intermediate logics are mainly technical.''

In a survey paper, Hosoi and Ono~\cite{hosoi-ono} says:
 \begin{quote}
  The study of intermediate logics seems to have two aspects: One is to
  study particularities of a certain logic, and the other is to take the
  intermediate logics as a whole and to study the relations between the
  logics or some structures recognized in that system.  We think that an
  intermediate logic is simply an algebraic system bearing some
  structural resemblance to the \textit{logic} in the usual sense and
  that it is not a \textit{logic} on which some kind of mathematics can
  or must be constructed.  So, the second approach seems to be
  reasonable for us, and we have been mostly working with the intension
  of grasping the albebraic structure of the whole system of the
  intermediate logic\footnote{Emphases by the authors of the original
  paper.}.
 \end{quote}
Their approach has been successful.
One spectacular result by Maksimova~\citep{maksimova77}
states that there are only seven consistent
superintuitionistic logics with the Craig interpolation
property.
The Craig interpolation property holds for a logic $L$ iff $\alpha
\imp\beta\in L$ implies existence of a formula $\chi$ such that
$\alpha\imp\chi, \chi\imp\beta\in L$ and $\chi$ only contains propositional
variables that appears in both $\alpha$ and $\beta$.
The seven logics include the intuitionistic propositional logic, the logic of
the weak excluded middle,
G\"odel--Dummett logic
and classical logic.
Although the Craig interpolation property is useful for program
verification~\citep{unno2009} \fix{cite others},
the above result itself is only of
theoretical interest.

We take the first approach described by Hosoi and Ono: namely,
studying particular logics instead of intermediate logics in general.
This thesis is about particular logics: G\"odel--Dummett logic and
monoidal t-norm logic.  We claim that these logics are usual logics
on which we can develop programming language.  The Curry--Howard
isomorphism is our justification of studying these particular logics.
G\"odel--Dummett logic is one typical intermediate logic.
Monoidal t-norm logic, on the other hand, is not an intermediate logic
because it lacks a structural rule called contraction.  Such logics are
called substructural logics, whose history will be followed in \ref{linear}.

We have to note that there has been some attempts to apply some
intermediate logics for mathematical reasoning, especially the truth
theory.
\citet{Hajek:TheJournalOfSymbolicLogic:2000} tried to use \L{}ukasiewicz
logic in order to resolve the liar's paradox.
The idea is assigning the truth value 0.5 to the sentence
``this sentence is false''.  When a sentence has
the truth value~$x$, a sentence that claims falsehood of the first
sentence should have the truth value $1-x$.  When the first and second
sentences are identical, it should have the truth value~$0.5$.
\citet{Hajek:TheJournalOfSymbolicLogic:2000} worked in Peano arithmetic
and found a consistent formulation (in the sense there exists a model to
each finite subtheory).
However, in the case of axiomatic set theory with comprehension,
\citet{hajek2005} showed that induction over natural numbers is contradictory.

Computationally interesting research on intermediate logics can be found
in proof searching.  For the typical intermediate logic G\"odel--Dummett
logic, there has been many proof searching implementations.
Currently, the fastest solver the author is aware of
is Fiorino's EPDL~\citep{Fiorino20103633}.
Fiorino benchmarked his and other implementations
 on a problem set for intuitionistic logic
called ILTP library~\citep{iltp}.
Related to proof searching,
Ferm\"uller~\cite{parallel} gave a game semantics for G\"odel--Dummett
logic, which is based on Lorenzen game~\cite{curryhoward} and essentially
proof searching bottom-to-up.
\fix{what communication is made?}

However, there has been no lambda calculi or programming languages based
on intermediate logics, apart from some cut-elimination results by
\citet{sonobe} and \citet{avron91}.


\subsection{Substructural Logics}
\label{linear}

We can understand the difference of the intuitionistic propositional
logic from classical
logic as the lack of contraction rule on the right hand side
\fix{really??}.

In sequent calculus or the sequent style natural deduction, when we
apply most rules, we need to make pattern-matching on the logical
formulae.
For example, in order to apply this $\land$L rule of sequent calculus
for classical logic,
 \begin{center}
 \aseq{\G,\phi,\psi}{\D}
 \LL{$\wedge$L}
 \useq{\G,\phi\land\psi}{\D}
  \DisplayProof
 \end{center}
we have to find a formula on the left hand
side sequent whose top connective is $\wedge$.

A structural rule\index{structural rule}
is a deduction rule that does not require
pattern-matching on formulae except two of them are equal or not.

Today, the logics without some structural rules are called substructural
logics\index{logic!substructural}.  There have been intensive studies on
substructural logics mainly from the algebraic
approach~\cite{residuated}.

\subsubsection{Logics without Exchange Rule}

Lambek calculus (\fix{cite lam58}) was originally developed for
analyzing natural language syntax.  When we modify Lambek calculus in
favor of syntactic mathematical logicians, we obtain FL.
All logics we treat in this thesis have exchange rules.

\subsubsection{BCI and BCK Logics}

When we add exchange rule to FL, we obtain BCI logic (originally called
BCC logic in \citep{ono-komori-1985}).
Further, when we add weakening to BCI logic, we obtain BCK
logic~\citep{ono-komori-1985}.
BCK logic\index{logic!BCK} has three axioms named after well-known
combinators.  Indeed, the logic can be characterized as the smallest set
closed by modus ponens and substitution that contains
\begin{description}
 \item[(B)] $(\phi\imp\psi)\imp((\chi\imp\phi)\imp(\chi\imp\psi))$
 \item[(C)] $(\phi\imp(\psi\imp\chi))\imp(\psi\imp(\phi\imp\chi))$ and
 \item[(K)] $\phi\imp(\psi\imp\phi)$\enspace.
\end{description}
From these, by modus ponens and substitution, (I) $\phi\imp\phi$
follows, but
(W) $(\psi\imp(\psi\imp\phi))\imp(\psi\imp\phi)$ does not follow.
Thus, the left hand side contraction is not admissible in the sequent
calculus for the BCK logic.

\subsubsection{Linear and Affine Logics}

Girard invented the linear logic~\citep{girard1987}.  It is a logic
without weakening or contraction.  There are two conjunctions
(multiplicative and additive) and two
disjunctions (again multiplicative and additive).
Depending on the formulation, there can be two
implications \fix{cite Troelstra}.
The multiplicative and additive operators differ in
the treatment of contexts in the branching rules.
\fix{elaborate}

In the intuitionistic linear logic, the right side of a sequent can only
contain at most one formula.

Abramsky~\citep{abramsky1993computational} gave computational
interpretations for the intuitionistic linear logic and the classical linear logic.


\subsubsection{Monoidal t-norm logic}

\citet{Esteva2001271} considered monoidal t-norm logic (MTL) for
\fix{what reason}


\subsection{Parametricity}

\subsubsection{Tait's Method}

Reynold

Logical Relation

Steps Index

\subsubsection{Classical Realizability}

Krivine

Curien

\subsection{Our Standing Point}

At the core of computer science lies the interplay of static formalism
and dynamic behaviour.  We can find examples in typed lambda calculi,
where static formalism of type derivations interacts with dynamic
behaviour of lambda terms.
Type derivations of static objects associtating lambda terms to types.
The reduction on terms gives dynamics, defining which term reduce
to which.  Static type derivations can guarantee dynamic properties of
programs such as strong normalization \fix{cite},
deadlock freedom \fix{cite}
and so on.

Dynamic behaviour involves time.
One simple notion of time is that of totally-ordered events where
one event happens before the other or the other before one \fix{cite something}.
This sentence is syntactically similar to Dummett's axiom that states
one proposition implies the other or the other implies one.
We investigate whether this syntactic similarity is reflected
in the dynamic semantics of logics: namely, the lambda calculi.

\fix{describe lambda}

Our investigation is guided by the Curry--Howard isomorphism.
We look at proofs as programs and propositions as types.
The Curry--Howard isomorphism is used to give computational content
to mathematical proofs, and on the other hand to give mathematical
guarantees to computer programs.


There have been several attempts of finding computational meaning of
Dummett's axiom.  \fix{name examples}
The best thing we can hope is that the computational
interpretation coincides with something we know already.
That is the case indeed.

We extended the Curry--Howard correspondence to G\"odel--Dummett logic.
The Curry--Howard correspondence is \fix{add}

G\"odel--Dummett logic is an intermediate logic~\citep{umezawa} between
classical logic and intuitionistic logic.  Every theorem in
intuitionistic logic is a theorem in G\"odel--Dummet logic and every
theorem in G\"odel--Dummett logic is a theorem in classical logic.
These inclusions are strict.  The law of excluded middle
$\varphi\vee(\varphi\supset \bot)$ is not a theorem in G\"odel--Dummett
logic and $(\varphi\supset\psi)\vee(\psi\supset\varphi)$ is not a
theorem in intuitionistic logic.

In terms of Kripke semantics, the axiom is sound and complete for
requiring that the models are linearly ordered.
In terms of algebraic semantics, the axiom is sound and complete for
requiring that the truth values are linearly ordered.
In terms of computation, what is it?

