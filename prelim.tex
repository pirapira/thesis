 \section{Preliminaries}

  We use ``iff\index{iff}'' as an abbreviation for ``if and only if.''
  We use set-theoretic concepts such as sets, relations and functions.
  For a set~$X$, $\powerset X$ denotes the powerset of $X$.
   $X\setminus Y$ denotes the subset of $X$ that consists of those
   elements of $X$ that are not elements of $Y$.
   Everywhere, we consider countably infinite set of logical formulae.
   A logic\index{logic} is a set of logical formulae, thus, there are at most
   $\powerset{\mathbb N}$ different logics once the set of logical
   formulae is fixed.
   Likewise, we consider at most countably many proofs and lambda terms.

   We denote partial maps as graphs.
   For example, $\{0\mapsto M, 1\mapsto N\}$ denotes a partial map that
   maps 0 to $M$ and 1 to $N$.
   We denote by $\dom(f)$ the domain of $f$, that is,
   the set of elements that are mapped to something.
   We also denote a partial map as a sequence whose index is the domain.
   For example, $(y_i)_{i\in I}$ denotes the partial map that maps
   $i\in I$ to $y_i$.
   For partial maps whose domains are disjoint, we use $\sqcup$ to
   denote the union.
   In other words, when $f$ and $g$ have disjoint domains, $f\sqcup g$
   denotes the
   partial map that maps $x$ to $f(x)$ or $g(x)$ when one of these is
   defined, or otherwise to nothing.

  For a symbol or a sequence of symbols $p$,
  $p^+$ denotes repetition of $p$ more than zero times and
  $p^*$ denotes repetition of $p$ more than or equal to zero times.
  \fix{are these notations ever used?}
  We use BNF (Backus Naur Form) for giving inductive definitions for sets
  of sequents of symbols.
 \begin{example}[BNF]
  When we say we define formulae~$\phi$ by BNF:
  \[
   \phi::= \bot \mid p\mid (\phi\imp\phi)
  \]
  where $p$ is a propositional variable, we actually define a set $\Phi$
  which is the smallest set such that
  \begin{itemize}
   \item $\bot$ is in $\Phi$
   \item each propositional variable is in $\Phi$ and
   \item if $\phi$ and $\psi$ are in $\Phi$, then $(\phi\imp\psi)$ is in $\Phi$.
  \end{itemize}
  and then we declare that we call elements of $\Phi$ formulae.
 \end{example}

 We take binary operators of the symbol $\imp$ or $\limp$,
 to be right associative.  For example, $\phi\limp\psi\limp\theta$
 is interpreted as $\phi\limp(\psi\limp\theta)$.

\section{History}

We briefly review the history of mathematical logic and computer science
to the extent relevant to the content of this thesis.
Especially, we focus on the treatments of Dummett's axiom $(a\imp
b)\lor(b\imp a)$ using the techniques in the history.
We confine ourselves to the developments of propositional logic and
ignore anything related to predicate logic or formalizing mathematics on
logics.  Nothing in this section originates from the current author.

\subsection{Birth of Formal Logic}

Implication is an important concept.
One treatment of implications called the material
implication\index{implication!material}.
The material implication can be traced back at least to
Frege's \textit{Begriffsschrift}~\citep{frege} in 1879.\index{Frege}
There, in the
section called ``conditionality,''
he begins by establishing four cases~\citep[p.~13]{frege}:
\begin{enumerate}
 \item $A$ is affirmed and $B$ is affirmed\,;
 \item $A$ is affirmed and $B$ is denied\,;
 \item $A$ is denied and $B$ is affirmed\,;
 \item $A$ is denied and $B$ is denied.
\end{enumerate}
Then he defines a notation involving $A$ and $B$
\[
\BGassert\BGconditional{B}{A}
\]
which ``stands for the
judgment that \textit{the third of these possibilities does not take
place, but one of the three other does\footnote{The emphasis is by
the English translation~\citep[p.~14]{frege}.}}''~\citep[p.~14]{frege}.
In the contemporary common mathematical terminology, this is equivalent
to stating $B$ implies $A$.
It was Russell who called this implication the material implication,
according to~\citet{sep-conditionals}.
After his notation, we still use $\vdash$ for judgements in our formal systems.

It is worth noting that Frege used the notation for implication to
define conjunction and
disjunction in the formal system, not the other way around.  Even in
today's treatments of
like algebraic semantics or categorical semantics, implication is
fundamental (orders in algebraic semantics, morphisms in categorical
semantics) and other logical connectives
can be defined using implication.

\subsection{Intuitionistic Propositional Logic}

The intuitionistic propositional logic\index{logic!intuitionistic
propositional}\index{intuitionistic propositional logic|see{logic}} is a logic, that is, a set of
logical formulae.
Although the name ``intuitionistic'' comes from Brouwer's
intuitionism\index{intuitionism},
the original claims of intuitionism are unrelated to this thesis.
Brouwer\index{Brouwer} was skeptical about the value of formalization of
mathematics and considered the studies of formal axiomatized logic as
``mathematics of the second and third order''%
~\citep[p.~10]{stigt1998}.
Nontheless, Brouwer approved publication of his student Arend Heyting's
work on defining a set of logical formulae as the intuitionistic
propositional logic.

\subsubsection{Heyting}
In 1930, Heyting~\cite{heyting1930}\index{Heyting} developed a deduction
system for the
intuitionistic propositional logic.  The presentation is similar to
the contemporary one except some notational differences.  The
logical connectives are the same $\{\wedge,\vee,\supset,\neg\}$ as today, the
first three binary and the last unary%
\footnote{Though, in this thesis, we prefer using nullary $\bot$ instead of unary
$\neg.$}%
.  Logical formulae are constructed
using these connectives and the propositional variables.
Although he sometimes used parentheses, he still used more points
around outer connectives (e.g. $b\supset \kern -3pt\cdot \kern 2pt  a
\supset b$ indicates the left $\supset$ is outer and the right one is
inner), but we use parentheses here.
These formulae are axioms\footnote{The axioms with numberings are taken
from \citet{heyting1930}.}, i.e. assumed to be
``correct formulae''~\cite{heyting1930}\index{formula!correct}:
\begin{description}
 \item[2.1.] $a\supset (a\land a)$
 \item[2.11.] $(a\land b)\supset (b\land a)$
 \item[2.12.] $(a\supset b)\supset (a\land c)\supset (b\land c)$
 \item[2.13.] $((a\supset b)\land (b\supset c))\supset (a\supset c)$
 \item[2.14.] $b\supset (a\supset b)$
 \item[2.15.] $(a\land (a\supset b))\supset b$
 \item[3.1.] $a\supset (a\lor b)$
 \item[3.11.] $(a\lor b)\supset (b\lor a)$
 \item[3.12.] $((a\supset c)\land(b\supset c))\supset ((a\lor b)\supset
      c)$
 \item[4.1.] $\neg a\supset (a\supset b)$
 \item[4.11.] $((a\supset b)\land (a\supset \neg b)) \supset \neg a$\enspace.
\end{description}
Moreover, there are methods to recognize more formulae to be correct
formulae~\cite{heyting1930}:
\begin{description}
 \item[1.2.] If $a$ and $b$ are correct formulas, then $a\land b$ is a correct
       formula.
 \item[1.3.] If $a$ and $b$ are correct formulas, then $b$ is a correct formula.
\end{description}

Following the intuitionists' belief
that ``it is in principle impossible to set up a system of formulas that
would be equivalent to intuitionistic mathematics, for the
possiblitities of thought cannot be reduced to a finite number of rules
set up in advance,''~\citep{heyting1930} he did not pursue completeness
results for the deduction system
except a remark mentioning Glivenko's theorem~\citep{glivenko0,glivenko1}.
However, he used a form of semantics in order to show that
each axiom is independent from the other axioms and that
the excluded middle is unprovable.
For example, in order to refute the excluded middle, he used the
following tables~\citep{heyting1930}:\\
 \begin{quotation}
 \begin{center}
  \begin{tabular}{c|ccc}
   $\supset $& 0  & 1  & 2 \\ \hline
   0 & 0 & 0 & 0 \\
   1 & 1 & 0 & 1 \\
   2 & 2 & 0 & 0
  \end{tabular}
  \hfill
  \begin{tabular}{c|ccc}
   $\wedge $& 0 & 1& 2\\ \hline
   0 & 0 & 1 & 2\\
   1 & 1 & 1 & 1\\
   2 & 2 & 1 & 2\\
  \end{tabular}
  \hfill
  \begin{tabular}{c|ccc}
   $\vee$& 0 & 1 & 2\\ \hline
   0 & 0 & 0 & 0 \\
   1 & 0 & 1 & 2 \\
   2 & 0 & 2 & 2\\
  \end{tabular}
  \hfill
  \begin{tabular}{c|ccc}
   $\neg $& 0 & 1 & 2\\ \hline
   & 1 & 0 & 1\\
  \end{tabular}\enspace.
 \end{center}
 \end{quotation}
When we assign one of $\{0,1,2\}$ to each propositional variable, we
can extend the assignment to all formulae using these tables.
For binary operators, the columns stand for the prearguments (the
arguments left of the operator) and the
rows stand for the postarguments (the arguments right of the operator).
For example, if we assign 1 to $a$ and 2 to $b$,
$a\supset b$ obtains value~0.
Under any assignment to propositional variables, all of Heyting's axioms
are assigned 0.  Moreover, these tables have two formidable properties:
\begin{enumerate}
 \item $0\land 0 = 0$
 \item $0\supset a$ has the value 0 only when $a = 0$.
\end{enumerate}
From these, all ``correct formulae'' in Heyting's deduction system
receive the value~0 under any assignments to propositional variables.
However, if we assign 2 to $a$, $(\neg \neg a)\supset a$
is assigned 2.  Thus, we can conclude that $\neg \neg a \supset a$ is
not a correct formula in Heyting's deduction system.
Note that the natural order between natural numbers
plays no rule here: it is not relevant that 2 is larger than 1 or 0 is
less than 1 (in other places he uses ``all positive and negative
whole numbers and 0'').  Although Heyting used natural numbers here, he
already obtained an equivalent notion of what is called Heyting algebra
today.

That is why we can refute Dummett's axiom $(a\supset b)\lor (b\supset
a)$\index{axiom!Dummett's} in the same method.
Indeed, according to these tables below, when we assign 1 to $a$ and 2
to $b$, $(a\supset b)\lor (b\supset a)$ obtains 4, which is not 0.
Since all correct formulae receive 0, $(a\imp b)\lor (b\imp a)$ is not a
correct formula.
 \begin{center}
  \begin{tabular}{c|ccccc}
   $\imp$ & 0& 1& 2& 3& 4\\ \hline
   0& 0& 0& 0& 0& 0\\
   1& 1& 0& 1& 0& 1\\
   2& 2& 2& 0& 0& 2\\
   3& 3& 3& 3& 0& 3\\
   4& 4& 0& 0& 0& 0
  \end{tabular}
  \hfill
  \begin{tabular}{c|ccccc}
   $\wedge$& 0& 1& 2& 3& 4\\ \hline
   0 & 0& 1& 2& 3& 4\\
   1& 1& 1& 3& 3& 1\\
   2& 2& 3& 2& 3& 2\\
   3& 3& 3& 3& 3& 3\\
   4& 4& 1& 2& 3& 4\\
  \end{tabular}
  \ruleskip
  \begin{tabular}{c|ccccc}
   $\vee$&0 &1 &2 &3 &4 \\ \hline
   0& 0& 0& 0& 0& 0\\
   1& 0& 1& 4& 1& 4\\
   2& 0& 4& 2& 2& 4\\
   3& 0& 1& 2& 3& 4\\
   4& 0& 4& 4& 4& 4\\
  \end{tabular}
  \hfill
  \begin{tabular}{c|ccccc}
   $\neg$& 0& 1& 2& 3& 4\\ \hline
   & 3& 3& 3& 0& 3\\
  \end{tabular}
 \end{center}

\subsubsection{G\"odel}
In 1932, G\"odel published a short note~\cite{godelprop} on
intuitionistic propositional
logic, where he proved two theorems: that the intuitionistic propositional logic
cannot be seen as a many-valued logic and that
there are infinitely many propositional logics between the
intuitionistic propositional logic and the ``ordinary'' (in today's
terminology, classical) propositional logic.  The second result is the
first contribution to the realm of intermediate
logics\index{logic!intermediate} (as
Troelstra~\cite[p.~223]{goedelcollected} put it).
For distinguishing those intermediate logics and the intuitionistic
propositional logic, he uses a formula~$F_n$ for each positive natural
number~$n$:
\[
 F_n = \bigvee_{1\le i < k\le n}\left((a_i\supset a_k) \land (a_k\supset a_i)\right)\enspace.
\]
On an $n$-element chain, $F_{n+1}$ is valid while $F_n$ might not be
satisfied.  As a result, no $F_n$ is valid in the intuitionistic
propositional logic.
Although Troelstra~\cite[p.~223]{goedelcollected} writes ``the reasons
for studying intermediate logics are mainly technical,'' we find that
one typical intermediate logic, G\"odel--Dummett logic, has a
computational interpretation which has already been known: waitfreedom.
The connection between G\"odel--Dummett logic and waitfreedom will be
treated in Chapter~\ref{ch:lambda}.

Among the formulae~$F_n$,
especially, the formula~$F_2$, which is $(a_1\supset a_2)\land (a_2\supset
a_1)$, brings contradiction into classical or intuitionistic logic.  If we
had $(p\land q)\supset p$ and $F_2$ as theorems in a logic closed under
substitution and modus ponens,
$a_1\supset a_2$ is also a theorem and all formulae would be theorems or
there would be no theorems.
% In Chapter~\ref{ch:exchange} we investigate a logic with an axiom similar to $F_2$:
% $(\phi\limp\psi)\otimes(\psi\limp\phi)$, where $\otimes$ is the
% multiplicative conjunction and $\limp$ is the linear implication.  Such
% attempt has been made possible by the development of linear and
% substructural logics explained below in \ref{linear}.
In short, in some of those settings, $(p\otimes q)\limp p$ is
not a theorem because we are not allowed to throw away $q$.

In the final sentence, \citet{godelprop} states what is known as
disjunction property\index{disjunction property} today: ``besides, the
following holds with full
generality: a formula of the form $A\lor B$ can only be provable in $H$
if either $A$ or $B$ is provable in $H$'', where $H$ is Heyting's calculus.
According to this statement, it is obvious that Dummett's axiom
$(p\supset q) \lor (q\supset p)$ is not provable.

\subsection{The Brouwer--Heyting--Kolmogorov Interpretation of Logical Connectives}

\subsubsection{Kolmogorov}

Thanks to Heyting~\cite{heyting1930}, we have a formal
characterization of the intuitionistic implication.
In 1932, Kolmogorov~\cite{kolmogorov1932} introduced a ``calculus of problems''
that coincides with the intuitionistic propositional logic.
In the calculus of problems, the logical connectives $\vee, \wedge,
\supset$ connect problems together to form another problem.
 \begin{quote}
  If $a$ and $b$ are two problems, then $a\land b$ designates the
  problem ``to solve both problems $a$ and $b$,'' while $a\lor b$
  designates the problem ``to solve at least one of the problems $a$ and
  $b$.''  Furthermore, $a\supset b$ is the problem ``to solve $b$
  provided that the solution for $a$ is given'' or, equivalently, ``to
  reduce the solution of $b$ to the solution of
  $a$.''  \ldots $\neg a$ designates the problem ``to obtain a
  contradiction provided that the solution of $a$ is
  given.''~\cite[p.~329]{kolmogorov1932}
 \end{quote}
 Here, the intuitionistic implication\index{implication!intuitionistic}
 is explained as reduction of problems.
 He continues to validate Heyting~\cite{heyting1930}'s axioms, and the
 deduction rules with respect to the problem calculus interpretation.

 Furthermore, he finds a rationale for not including the law of excluded
 middle $a\lor \neg a$ by saying ``one must possess a general method
 either to prove or to reduce to a contradiction any proposition.  If
 our reader does not consider himself to be omniscient, he will probably
 determine that the formula cannot be found on the list of problems
 solved by him''~\cite{kolmogorov1932}.
 This argument is enough to reject Dummett's axiom $(a\supset b)\lor
 (b\supset a)$ because one must possess a general method, given any two
 problems, to
 determine whether
 one problem can be reduced to the other or the other way around.
 For the computational meaning of the law of excluded middle, we have to
 wait until 1990's.
 And the computational interpretation of Dummett's axiom is
 presented in Chapter~\ref{ch:lambda} in this thesis.

\subsubsection{Realization as Typed Lambda Terms}

The Brouwer--Heyting--Kolmogorov interpretation reads: ``a proof of the
implication $\varphi\supset\psi$ is a construction which permits us to
 transform any proof of $\varphi$ into a proof of $\psi$''~\fix{cite}.
The BHK
interpretation does not specify what is a proof or what kind of
transformation witnesses implication.
Notwithstanding, there is a
traditional realization of proofs as lambda terms where the
introduction rule of implication is realized as the lambda abstraction
and the elimination rule of implication is realized as the application
of lambda terms.
This encoding has a formidable property:
the reductions in the lambda calculus corresponds to the reductions of
proofs that
removes detours.
For example, suppose a natural deduction proof introduces an implication and then
immidiately eliminate the
implication.  This proof can be encoded as a lambda term whose outermost
structure is a $\beta$-redex: $(\lambda x. M)N$.
 The result of the $\beta$-reduction $M[N/x]$ encodes a proof tree using
 the same assumptions as the original and concluding the same formula as
 the original, yet without the aforementioned detour.
 a proof of implication allows transformation of proofs
by means of
substitution.  The encoding of proofs as lambda terms is traditionally
called the Curry--Howard isomorphism.
% The situation is similar to axiomatized geometry where lines and points
% could be tables and chairs \fix{(cite)}.
The former explanation of implications is justified in classical propositional logic.
The latter view is most naturally materialized in the situation of
intuitionistic propositional logic.  The last century saw their
generalization called intermediate logics~\citep{umezawa} (or superintuitionistic
logics\index{logic!superintuitionistic}), of which a typical example is
called G\"odel--Dummett logic.
In this thesis we investigate the Curry--Howard isomorphism for
G\"odel--Dummett logic.  The G\"odel--Dummett
logic\index{logic!G\"odel--Dummett} validates formulas
of the form $(\phi\imp\psi)\lor(\psi\imp\phi)$, which is known as
Dummett's axiom\index{axiom!Dummett's}\index{Dummett's
axiom|see{axiom}}.  We add a construct to the simply typed lambda
calculus
that witnesses Dummett's axiom.
After the intermediate logics, the generalization went further to
substructural
logics~\citep{residuated}, which contains all the intermedaite logics as
well as the
(intuitionistic) multiplicative
additive fragment of linear logic.
In the later chapter, we make a typing system that lacks contraction and
weakening, which falls outside of intermediate logics but within
substructural logics albeit it has quantification over propositional variables.


\subsection{Natural Deduction and Sequent Calculus}

\subsubsection{Gentzen}\index{Gentzen}

The deduction system of \citet{heyting1930} characterizes what is still
called intuitionistic propositional logic today. However, Heyting's
system has one drawback, which is shared with almost all other
Hilbert-style derivation systems:
in order to prove a logical formula, sometimes we have to
mention a larger, more complicated formulae.  Actually, there
are alternative formulations of intuitionistic (and classical)
propositional logic by \citet{gentzen} where we only have to
mention subformulae of the formula being proven.
The alternative proof systems are called natural deduciton\index{natural
deduction} and sequent calculus\index{sequent calculus}.

In contrast to the sytem of \citet{heyting1930} where a proof yields a
finite set of ``correct formulae'', a proof in natural deduction and
sequent calculus yields a pair of assumptions and conclusions.
\citet{gentzen} used the form
\[
 \mathbf{A_1},\ldots,\mathbf{A_{\boldsymbol\mu}}\longrightarrow \mathbf{B}
\]
as a sequent.  We will use $\vdash$ instead of $\longrightarrow$.

In natural deduction, the assumptions and conclusions are presented
vertically.  For example,
 \begin{center}
  \AxiomC{$A\land B$}
  \UnaryInfC{$B$}
  \DisplayProof
 \end{center}
 has a single assumption $A\land B$ and a conclusion~$B$.
 The horizontal line shows an application of an inference rule%
 \footnote{In \citep{gentzen}, an inference rule is called an inference
 figure schema.}.
Natural deduction has introduction rules and elimination rules for each
logical connectives.  For example, the introduction and elimination
rules for $\wedge$
 (conjunction\footnote{\citet{gentzen} and \citet{prawitz1965} used
 $\&$.}) is as follows:
 \[
  \fix{fill} \fix{fill} \fix{fill}
 \].
In later chapters, we will use natural deduction for presenting logics.

\subsubsection{Prawitz}

\citet{gentzen} did not prove the subformula property immediately for
natural deduction.  He proved the subformula property for sequent
calculus first and then after that obtained the property of natural
deduction as a corollary.
On the other hand, \citet{prawitz1965} studied natural deduction in
itself.

Inversion principle states that an occurrence of a logical connective
in a proof can be removed when the connective is introduced by
an introduction rule, and then immediately below, eliminated by an
elimination rule.  A logical formula occurrence containing such a
connective is called a maximal formula.
For example, for conjunction, a natural deduction
derivation~\citep[p.~36]{prawitz1965} can be reduced
 \begin{center}
  from
 \AxiomC{$\Sigma_0$}
  \UnaryInfC{$A$}
 \AxiomC{$\Sigma_1$}
  \UnaryInfC{$B$}
  \LL{$\wedge\intro$}
  \BinaryInfC{$A\land B$}
  \LL{$\wedge\elim$}
  \UnaryInfC{$A$}
  \DisplayProof
  to
 \AxiomC{$\Sigma_0$}
  \UnaryInfC{$A$}
  \DisplayProof
 \end{center}
 where $\Sigma_0$ and $\Sigma_1$ stand for derivations with conclusions
 $A$ and $B$ respectively.
 In the reduction, the occurrence of $\wedge$ is removed.
 The same holds for other logical connectives: thus we can remove
 maximal formulae.

 Further, \citet[Chapter~IV]{prawitz1965} formulated a weaker notion
 called maximal segments.  Above, in the definition of maximal formulae,
 the elimination rule must be placed immediately below the corresponding
 introduction rule.  Prawitz allowed other inference lines between the
 introduction and elimination rules and defined maximal segments.
 A derivation without maximal segments is called normal.
 And Prawitz proved the existence of a normal derivation of $\G\vdash A$
 given any derivation of $\G\vdash A$.
 \citet{prawitz1965} treated first-order, second-order and modal logics but
 we do not elaborate.

 The reductions and the normal form coincide with the $\beta$-reduction
 and the normal form in the typed lambda
 calculi under Curry--Howard isomorphism.


\subsection{Curry--Howard Isomorphism}

The Curry--Howard isomorphism\index{Curry--Howard isomorphism}
 is originally a correspondence between
intuitionistic propositional logic proofs and typed lambda terms, but
the technical term has obtained a more general meaning containing
correspondence between proofs and programs in general.
The phrase ``computational interpretation'' is reserved for
the Curry--Howard isomorphism.
\citet{curryhoward} recently presented a comprehensive reference on the
topic, containing lots of historical and bibliographical remarks.

\subsubsection{Curry}
The Curry--Howard isomorphism is originally the correspondence of
the typed lambda terms and the proofs of the implicational fragment of
the intuitionistic propositional logic.
According to \citep{curryhoward}, the first explicit statement of the
correspondence appears
in the retiring presidential address to the Association for Symbolic
Logic titled ``the combinatory foundations of mathematical
logic''~\cite{curry1942}\index{Curry}.
The footnote~28 of \citep{curry1942} reads:
 \begin{quote}
  Note the similarity of the postulates for $F$ and those for $P$.  If
  in any of the former postulates we change $F$ to $P$ and drop the
  combinator we have the corresponding postulate for $P$.
 \end{quote}
 On the same page, there is Postulate~(PC):
\[
 \vdash (\beta\supset (\alpha\supset \gamma))\supset (\alpha \supset
 (\beta\supset \gamma))\enspace.
\]
And a corresponding Postulate~(FC):
\[
 \vdash F(F\beta(F\alpha\gamma))(F\alpha(F\beta\gamma))C
\]
 where $C\equiv \lambda^3 xyz\cdot xzy$ and $F\alpha\beta$ shows the
 functional character of a
 function from $\alpha$ to $\beta$.  The postulate~(FC) is said to
 state the functional character of $C$.
 In this thesis, we use sequent style natural deduction system so that these
 postulates can be derived as
 \[
 \AxiomC{}
 \useq{\tj{x}{\beta\supset (\alpha\supset \gamma)}}{\tj{x}{\beta\supset
 (\alpha\supset \gamma)}}
 \AxiomC{}
 \useq{\tj{z}{\beta}}{\tj{z}{\beta}}
 \bseq{\tj{x}{\beta\supset (\alpha\supset \gamma)},
 \tj{z}{\beta}}{\tj{xz}{\alpha\supset\gamma}}
 \AxiomC{}
 \useq{\tj{y}{\alpha}}{\tj{y}{\alpha}}
 \bseq{\tj{x}{\beta\supset (\alpha\supset \gamma)},
 \tj{z}{\beta},\tj{y}{\alpha}}{\tj{xzy}{\gamma}}
 \useq{\tj{x}{\beta\supset (\alpha\supset
 \gamma)},\tj{y}{\alpha}}{\tj{\lambda z. xzy}{\beta\supset\gamma}}
 \useq{\tj{x}{\beta\supset (\alpha\supset
 \gamma)}}{\tj{\lambda y.\lambda z. xzy}{\alpha\supset(\beta\supset\gamma)}}
 \useq{}{\tj{\lambda x.\lambda y.\lambda z. xzy}{{(\beta\supset (\alpha\supset
 \gamma))}\supset (\alpha\supset(\beta\supset\gamma))}}
 \DisplayProof \enspace .
 \]
 Indeed, the last sequent associates the term $\lambda x.\lambda
 y.\lambda z. xzy$, which is the combinator $C$, to the logical formula
 $(\beta\supset (\alpha\supset
 \gamma))\supset (\alpha\supset(\beta\supset\gamma))$.

The
precise statements for the correspondence appear in
\citet[9E]{curry1974combinatory}, a section titled ``analogies with
propositional algebra.''
\fix{cite the statements}

 Henceforth, we do not give deduction system of a logic and
 typing rules for a typed lambda calculi separately because
 the former can be obtained from the lattar by this correspondence.

There, Curry notes the correspondence is not isomorphism\fix{or, in
sorenzen's book ???}  We take a finer approach.

\subsubsection{Application to Programming Languages}

\citet{landin1965} noticed some similarities between ALGOL~60 syntax and
the untyped lambda calculus.
\fix{survey ML like languages}

However, none has employed Dummett's axiom as a type for a language primitive.

\subsection{G\"odel--Dummett Logic}
\subsubsection{Dummett}
Dummett~\cite{dummett59} considers a semantics on $\{0,1,2,\ldots,\omega\}$ where
$\wedge$ is interpreted as maximum, $\vee$ as minimum, $\bot$ as $\omega$
and conjunction~$\wedge$
 and implication as a
function~$\hat\supset$ where%
\footnote{To be precise, Dummett did not use absurdity~$\bot$ but negation~$\neg$
although they are interdefinable in existence of implication~$\supset$
and conjunction~$\wedge$.}
\begin{align*}
 x \hat\supset y= \begin{cases}
		    0 &\text{ if } x\ge y \\
		    y &\text{ if } x < y
		  \end{cases}\enspace.
\end{align*}
He axiomatized the logic by adding $(p\supset q)\lor(q\supset p)$ on top
of the axioms for the intuitionistic propositional logic.
Thomas~\citep{thomas1962} axiomatized
 the logic on $n$-element chains using Dummett's axiom and the
 formula~$F_{n+1}$,
 which is rewritten using only implications but no disjunction or conjunction.

\subsubsection{Sonobe}
\fix{say something}

\subsubsection{Avron}
Hypersequent


\subsubsection{Later Developments}

\subsection{Waitfreedom}

\subsubsection{Lamport}

\subsubsection{Gafni}

\subsubsection{Herlihy and Shavit}

\subsubsection{Saks and Zaharoglow}

\subsection{Superintuitionistic (Intermediate) Logics}

A logic is a set of logical formulae which is closed by
substitution and modus ponens.
 A superintuitionistic logic is a logic
that contains the intuitionistic propositional logic.  A logic is consistent when it does not
contain all logical formulae.

Troelstra~\cite[p.~223]{goedelcollected} writes ``the reasons
for studying intermediate logics are mainly technical.''

In a survey paper, Hosoi and Ono~\cite{hosoi-ono} says:
 \begin{quote}
  The study of intermediate logics seems to have two aspects: One is to
  study particularities of a certain logic, and the other is to take the
  intermediate logics as a whole and to study the relations between the
  logics or some structures recognized in that system.  We think that an
  intermediate logic is simply an algebraic system bearing some
  structural resemblance to the \textit{logic} in the usual sense and
  that it is not a \textit{logic} on which some kind of mathematics can
  or must be constructed.  So, the second approach seems to be
  reasonable for us, and we have been mostly working with the intension
  of grasping the albebraic structure of the whole system of the
  intermediate logic\footnote{Emphases by the authors of the original
  paper.}.
 \end{quote}
Their approach has been successful.
One spectacular result by Maksimova~\citep{maksimova77}
states that there are only seven consistent
superintuitionistic logics with the Craig interpolation
property.
The Craig interpolation property holds for a logic $L$ iff $\alpha
\imp\beta\in L$ implies existence of a formula $\chi$ such that
$\alpha\imp\chi, \chi\imp\beta\in L$ and $\chi$ only contains propositional
variables that appears in both $\alpha$ and $\beta$.
The seven logics include the intuitionistic propositional logic, the logic of
the weak excluded middle,
G\"odel--Dummett logic
and classical logic.
Although the Craig interpolation property is useful for program
verification~\citep{unno2009} \fix{cite others},
the above result itself is only of
theoretical interest.

We take the first approach described by Hosoi and Ono: namely,
studying particular logics instead of intermediate logics in general.
This thesis is about particular logics: G\"odel--Dummett logic and
monoidal t-norm logic.  We claim that these logics are usual logics
on which we can develop programming language.  The Curry--Howard
isomorphism is our justification of studying these particular logics.
G\"odel--Dummett logic is one typical intermediate logic.
Monoidal t-norm logic, on the other hand, is not an intermediate logic
because it lacks a structural rule called contraction.  Such logics are
called substructural logics, whose history will be followed in \ref{linear}.

We have to note that there has been some attempts to apply some
intermediate logics for mathematical reasoning, especially the truth
theory.
\citet{Hajek:TheJournalOfSymbolicLogic:2000} tried to use \L{}ukasiewicz
logic in order to resolve the liar's paradox.
The idea is assigning the truth value 0.5 to the sentence
``this sentence is false''.  When a sentence has
the truth value~$x$, a sentence that claims falsehood of the first
sentence should have the truth value $1-x$.  When the first and second
sentences are identical, it should have the truth value~$0.5$.
\citet{Hajek:TheJournalOfSymbolicLogic:2000} worked in Peano arithmetic
and found a consistent formulation (in the sense there exists a model to
each finite subtheory).
However, in the case of axiomatic set theory with comprehension,
\citet{hajek2005} showed that induction over natural numbers is contradictory.

Computationally inclined research on intermediate logics can be found
in proof searching.  For the typical intermediate logic G\"odel--Dummett
logic, there has been many proof searching implementations.
Currently, the fastest solver the author is aware of
is Fiorino's EPDL~\citep{Fiorino20103633}.
Fiorino benchmarked his and other implementations
 on a problem set for intuitionistic logic
called ILTP library~\citep{iltp}.
Related to proof searching,
Ferm\"uller~\cite{parallel} gave a game semantics for G\"odel--Dummett
logic, which is based on Lorenzen game~\cite{curryhoward} and essentially
proof searching bottom-to-up.
\fix{what communication is made?}

However, there has been no lambda calculi or programming languages based
on intermediate logics, apart from some cut-elimination results by
\citet{sonobe} and \citet{avron91}.


\subsection{Substructural Logics}
\label{linear}

We can understand the difference of the intuitionistic propositional
logic from classical
logic as the lack of contraction rule on the right hand side
\fix{really??}.

In sequent calculus or the sequent style natural deduction, when we
apply most rules, we need to make pattern-matching on the logical
formulae.
For example, in order to apply this $\land$L rule of sequent calculus
for classical logic,
 \begin{center}
 \aseq{\G,\phi,\psi}{\D}
 \LL{$\wedge$L}
 \useq{\G,\phi\land\psi}{\D}
  \DisplayProof
 \end{center}
we have to find a formula on the left hand
side sequent whose top connective is $\wedge$.

A \textit{structural rule}\index{structural rule}
is a deduction rule that does not require
pattern-matching on formulae except two of them are equal or not.

Today, the logics without some structural rules are called \textit{substructural
logics}\index{logic!substructural}.  There have been intensive studies on
substructural logics mainly from the algebraic
approach~\cite{residuated}.

\subsubsection{BCI and BCK Logics}

When we add exchange rule to FL, we obtain BCI logic (originally called
BCC logic in \citep{ono-komori-1985}).
Further, when we add weakening to BCI logic, we obtain BCK
logic~\citep{ono-komori-1985}.
\textit{BCK logic}\index{logic!BCK} has three axioms named after well-known
combinators.  Indeed, the logic can be characterized as the smallest set
closed by modus ponens and substitution that contains
\begin{description}
 \item[(B)] $(\phi\imp\psi)\imp((\chi\imp\phi)\imp(\chi\imp\psi))$
 \item[(C)] $(\phi\imp(\psi\imp\chi))\imp(\psi\imp(\phi\imp\chi))$ and
 \item[(K)] $\phi\imp(\psi\imp\phi)$\enspace.
\end{description}
From these, by modus ponens and substitution, (I) $\phi\imp\phi$
follows, but
(W) $(\psi\imp(\psi\imp\phi))\imp(\psi\imp\phi)$ does not follow.
Thus, the left hand side contraction is not admissible in the sequent
calculus for the BCK logic.

\subsubsection{Linear and Affine Logics}

Girard invented the linear
logic~\citep{girard1987}\index{logic!linear}\index{linear
logic|see{logic}}.
To explain this logic, let us present two formulations of
the $\wedge$-right rule in the sequent calculus of intuitionistic
propositional logic:
 \begin{center}
  \AxiomC{$\G\tr\phi$}
  \AxiomC{$\D\tr\psi$}
  \BinaryInfC{$\G,\D\tr\phi\land\psi$}
  \DisplayProof
  \hskip 3cm
  \AxiomC{$\G\tr\phi$}
  \AxiomC{$\G\tr\psi$}
  \BinaryInfC{$\G\tr\phi\land\psi$}
  \DisplayProof
 \end{center}
 Using one of these rule, the other can be defined as a macro
 (an abbreviation for a longer construction)
 because, from bottom to top, we can copy
 (\textit{contraction}\index{contraction}) or remove
 (\textit{weakening}\index{weakening}) formulae in contexts.
 However if we do not allow such structural rules,
 the two different formulations characterize different conjunctions.
 The left one is called \textit{multiplicative}\index{multiplicative}
 conjunction and the right one is called
 \textit{additive}\index{additive}
 conjunction.
 \begin{center}
  \AxiomC{$\G\tr\phi$}
  \AxiomC{$\D\tr\psi$}
  \BinaryInfC{$\G,\D\tr\phi\otimes\psi$}
  \DisplayProof
  \hskip 3cm
  \AxiomC{$\G\tr\phi$}
  \AxiomC{$\G\tr\psi$}
  \BinaryInfC{$\G\tr\phi\with\psi$}
  \DisplayProof
 \end{center}

 In general, a binary operator is called multiplicative (resp.~additive)
 when the right rule in sequent calculus (or introduction rule in
 natural deduction) for the operator splits the context (resp. copies
 the whole context in all branches). \fix{introduce ``context''
 somewhere before this}
 The example above shows how the additive and multiplicative
 conjunctions are different.
 The words multiplicative and addtive are originally adjectives but
 we sometimes use them as nouns: ``multiplicatives'' for the
 multiplicative operators and ``additives'' for the additive operators.

 % Depending on the formulation, there can be two
 % implications \fix{cite Troelstra}.

 In the intuitionistic linear logic, the right side of a sequent can only
 contain at most one formula.  In this thesis we are in this
 setting.
 Abramsky~\citep{abramsky1993computational} gave computational
 interpretations for the intuitionistic linear logic and the classical
 linear logic.  Since we will extend his linear lambda calculus in
 Chapter~\ref{ch:exchange}, we are going to elaborate on this there.

 \subsubsection{Proof Nets}

 We are going to treat the intuitionistic version of
 multiplicative linear logic, and its proof nets are based on Amida
 lotteries.

\subsubsection{Monoidal t-norm logic}

The linear version of G\"odel--Dummett logic is called the monoidal
t-norm logic (MTL), which validates an axiom
$(\phi\limp\psi)\oplus(\psi\limp\phi)$.

\citet{Esteva2001271} considered monoidal t-norm logic (MTL) for
\fix{what reason}

In Chpater~\ref{ch:exchange}, we study a similar axiom
$(\phi\limp\psi)\otimes(\psi\limp\phi)$.
For comparison, we develop a hyper-lambda calculus for MTL in
Appendix~\ref{ch:pole}.


% \subsection{Parametricity}

% \subsubsection{Tait's Method}

% Reynold

% Logical Relation

% Steps Index

% \subsubsection{Classical Realizability}

% Krivine

% Curien

% \subsubsection{Summary}

% Omitted

% focusng

% sara negri and von plato

% expansion to modal logics

% tradition of game semantics

\section{Significance of Our Contributions}

At the core of computer science lies the interplay of static formalism
and dynamic behaviour.  We can find examples in typed lambda calculi,
where static formalism of type derivations interacts with dynamic
behaviour of lambda terms.
Type derivations of static objects associtating lambda terms to types.
The reduction on terms gives dynamics, defining which term reduce
to which.  Static type derivations can guarantee dynamic properties of
programs such as strong normalization \fix{cite},
deadlock freedom \fix{cite}
and so on.

Dynamic behaviour involves time.
One simple notion of time is that of totally-ordered events where
one event happens before the other or the other before one \fix{cite something}.
This sentence is syntactically similar to Dummett's axiom that states
one proposition implies the other or the other implies one.
We investigate whether this syntactic similarity is reflected
in the dynamic semantics of logics: namely, the lambda calculi.

\fix{describe lambda}

Our investigation is guided by the Curry--Howard isomorphism.
We look at proofs as programs and propositions as types.
The Curry--Howard isomorphism is used to give computational content
to mathematical proofs, and on the other hand to give mathematical
guarantees to computer programs.


There have been several attempts of finding computational meaning of
Dummett's axiom.  \fix{name examples}
The best thing we can hope is that the computational
interpretation coincides with something we know already.
That is the case indeed.

We extended the Curry--Howard correspondence to G\"odel--Dummett logic.
The Curry--Howard correspondence is \fix{add}

G\"odel--Dummett logic is an intermediate logic~\citep{umezawa} between
classical logic and intuitionistic logic.  Every theorem in
intuitionistic logic is a theorem in G\"odel--Dummet logic and every
theorem in G\"odel--Dummett logic is a theorem in classical logic.
These inclusions are strict.  The law of excluded middle
$\varphi\vee(\varphi\supset \bot)$ is not a theorem in G\"odel--Dummett
logic and $(\varphi\supset\psi)\vee(\psi\supset\varphi)$ is not a
theorem in intuitionistic logic.

In terms of Kripke semantics, the axiom is sound and complete for
requiring that the models are linearly ordered.
In terms of algebraic semantics, the axiom is sound and complete for
requiring that the truth values are linearly ordered.
In terms of computation, what is it?

Each chapter can be read independently.

\subsection{\fix{chapter two}}
\subsection{\fix{chapter three}}
\subsection{\fix{chapter four}}

